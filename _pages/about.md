---
permalink: /
title: ""
excerpt: ""
author_profile: true
redirect_from: 
  - /about/
  - /about.html
---

About me
======
* I am currently working as an assistant professor, collaborating with Prof. [[Zhiming Zheng]](https://iai.buaa.edu.cn/info/1013/1088.htm) and Prof. [[Wenjun Wu]](https://iai.buaa.edu.cn/info/1013/1093.htm), at School of Artificial Intelligence, Beihang University.  

* I pursued my PhD from 2019 to 2024 across three prestigious institutions: [Carnegie Mellon University](https://www.cmu.edu), [The Hong Kong University of Science and Technology](https://www.ust.hk), and [Renmin University of China](https://www.ruc.edu.cn), studying Computer Science. At CMU, I had the privilege of working under the guidance of Prof. [Min Xu](https://xulabs.github.io/min-xu/). My time at HKUST was spent collaborating with Prof. [Kai Chen](https://cse.hkust.edu.hk/~kaichen/). While at RUC, I worked closely with Prof. [Jun He](http://info.ruc.edu.cn/jsky/rtjs/index.htm) and Prof. [Hongyan Liu](https://www.sem.tsinghua.edu.cn/info/1210/32067.htm) from Tsinghua University. My research interests lie in the areas of multi-modal LLMs, avatars, and embodied AI.Please feel free to contact me by  email (fanzhaoxinruc@gmail.com or zhaoxinf@buaa.edu.cn), if you are interested in my research.
  

Our team is dedicated to realizing the vision of **embodied artificial intelligence**, with a clear roadmap starting from **scientific embodiment** and progressing towards **general embodied intelligence**. Our first goal is to achieve **scientific embodiment**: empowering robots to proficiently perform scientific experiments, including precise manipulation, observation, measurement, and use of laboratory equipment. This requires not only advanced motor skills, but also the capability to interact intelligently with complex scientific instruments and environments.

To support this, we explore:

- **MLLMs (Multimodal Large Language Models):**
  We develop MLLMs to study and replicate the low-energy, highly efficient "big and small brain" mechanisms of humans (cerebrum and cerebellum), providing the core intelligence for our robots.
- **Avatar (Digital Human Simulation):**
  By simulating human behaviors and interactions in virtual environments, we can study, predict, and refine how robots should behave in real-world scientific scenarios, accelerating the transfer of knowledge from simulation to reality.
- **Scientific Embodiment:**
  We integrate the above advances to create robots purpose-built for complex scientific experiments, setting a new standard for intelligence and utility in laboratory automation.

Looking ahead, our ultimate vision is **general embodied intelligence**: building robots that can master and utilize any human tool, extending far beyond the laboratory into all domains where tool use and embodied reasoning are needed.

---

Through this stepwise approach—from **scientific embodiment** as the initial breakthrough, to **general embodied intelligence** as the final goal—we aim to push the boundaries of artificial intelligence and robotics.
  



* If you are interested in becoming my student or a close collaborator, please answer the following questions before contacting me via email:
  * Firstly, familiarize yourself with my research area by reading my recent papers and visiting our research group's webpage. Research requires passion, and lack of interest may hinder persistence.
  * If you are applying for a direct PhD or a combined Master's and PhD program, it implies that you see academia as a career path. Please decide after understanding the challenges and rewards of an academic life.
  * When applying for a Master's program, I value excellent undergraduate grades, a solid foundation in mathematics, proficient programming skills, and good English proficiency.
  * I hope you are optimistic and proactive, with a resilient spirit, clear logical thinking, and good teamwork skills.
  * After considering these points, if you still wish to choose me as your mentor, please send your resume via email, including personal details, academic records, work or internship experiences, research experience, publications, and any other relevant information. I will respond as soon as possible.
 


<br>

News!
======
*  Feb/20/2026,   Excited to annouce that 5 Papers are accepted to **CVPR 2026** !
*  Jan/26/2026,  Glad to annouce that 3 papers are accepted to  **ICLR 2026**!
*  Jan/20/2026,  I am happy to serve as the **Area Chair of ICME 2026**!
*  Jan/14/2026,  Glad to annouce that DSSmoothing is accepted to  **WWW 2026**!.
*  Dec/20/2025, Congrats to Ye for his first VLA work being accepted by **Neurocomputing**!
*  Dec/3/2025,  Glad to annouce that R-FGDepth is accepted to  **Pattern Recongition**!.
*  Nov/26/2025,  Congrats to Xinyu for his first theory work on pruning being accepted by **Neurocomputing**!
*  Nov/8/2025,  Glad to annouce that Monodream and Mem4d are accepted to **AAAI 2026** ! 
*  Oct/25/2025,  Excited to annouce that SyncTalk++ is accepted to **TPAMI** !
*  Oct/16/2025,  Excited to annouce that AsynFusion is awarded by PRCV 2025 **best student paper** and **CCF outstand paper**! 
*  Sep/24/2025,  Glad to annouce that 1 paper is accepted to **Neurips**.
*  Sep/13/2025,  Glad to annouce that our latest survey paper on avatars is accept to **ACM Computing Surveys！**.
*  Sep/1/2025,  Glad to annouce that 1 paper is accepted to **Pattern Recognition**. The impact factor of PR is 7.6 now.
*  Aug/21//2025,  Glad to annouce that 1 paper is accepted to **PRCV 2025**. 
*  Aug/2/2025,  Excited to annouce that LongVLA is accepted to **CoRL2025** !
*  July/5/2025,  Excited to annouce that 2 papers are accepted to **ACM MM 2025** !
*  June/26/2025,  Excited to annouce that 2 papers are accepted to **ICCV 2025** !
*  June/22/2025,  Glad to annouce that TBA is accepted to **Pattern Recongition**. Our first work on avatar security!
*  June/8/2025,  I am happy to serve as the **Area Chair of 3DV 2026**!
*  May/20/2025,  I am happy to serve as the **Area Chair of PRCV 2025**!
*  May/15/2025,  Excited to annouce that 2 papers are accepted to **ACL 2025** !
*  May/1/2025,  EraseAnything is accepted to **ICML 2025！**  Congrats to all co-authors!
*  April/29/2025,  GLDiTalkerg is accepted to **IJCAI 2025！** Congrats to all co-authors!
*  March/30/2025,  We are hiring RAs at HKUST! Welcome to contact us by zhaoxinf@buaa.edu.cn!
*  March/21/2025,  Two paper are accepted to **ICME 2025**. Congrats to Xukun and Zhiying!
*  Feb/27/2025,  Excited to annouce that 3 papers are accepted to **CVPR 2025** !
*  Feb/12/2025,  Excited to annouce that VarGes is  accepted to **CVMJ**. The impact factor of CVMJ is 17.3.
*  Dec/1/2024,  Glad to annouce that Idea-2-3d is accepted to **Coling 2025** main track.
*  July/1/2024,  Glad to annouce that MLPHand is accepted to **ECCV 2024** main track.
*  April/7/2024,  Excited to annouce that 5 papers are accepted to **ICMR 2024**.
*  March/22/2024,  Congrats to Han Sun for his first paper being accepted to **IEEE Transactions on Instrumentation & Measurement.**
*  Feb/27/2024,  Glad to annouce that SyncTalk is accepted to **CVPR 2024** main track.
*  Dec/9/2023,  Our recent paper Everything2Motion is accepted to **AAAI 2024** main track and the paper FurPE is accepted to AAAI 2024 workshop.  Congrats to all authors.
*  Nov/17/2023,  Congrats to Yixing Lu for his first paper being accepted to **International Conference on Multimedia Modeling**.
*  Sep/21/2023,  We are awarded the second prize in the National Challenge Cup! I together with Professor [Li Ronghua](https://ronghuali.github.io/ronghuali.html) serve as the advisors of this project.
*  July/26/2023, I am thrilled to announce that our latest work, SelfTalk, has been accepted by **ACM MM 2023!**
*  Two papers D-IF and Emo-Talk are accepted by **ICCV 2023**, one of the best conference in computer vision.
*  Excited to annouce that one of our  papers has been accepted by **signal processing**, IF=4.729
*  Excited to annouce that one of my papers has been accepted by **IJCAI 2023**.
*  I am honored to have returned to Carnegie Mellon University as a visiting scholar.
*  One of my papers has been accepted for presentation at **CVPR 2023**, which is considered one of the most prestigious computer vision conferences in the world.
*  I am thrilled to have had one of my papers accepted for presentation at **ICRA 2023**, which is a leading robotics conference.
*  My research has been recognized at **ECCV Workshop 2022**, **PRCV 2022**, and **ECCV 2022**, all of which are highly-regarded international conferences in the field of computer vision.
*  I am proud to have had a paper accepted for publication in the **ACM Computing Surveys** journal, which is widely considered one of the best ACM journals with an impact factor of 14.324.
*  The virtual actor An-Ruohan of Psyai, which I helped to develop, performs her show daily on bilibili.
*  My research has also been recognized at **ICIP 2022**, which is a top-tier computer vision conference with a long-standing reputation for excellence.
*  I am excited to announce that one of my papers has been accepted for presentation at **AAAI 2022**, which is a leading artificial intelligence conference.



<br>


Working Experience
======
* Researcher, Psyche AI Inc (2021 - 2024)
  * As a researcher at Psyche AI Inc, I am responsible for leading and overseeing various projects related to human body reconstruction, talking head technology, and 3D foundation models. In this role, I contribute my expertise in the development of innovative AI solutions that advance the field of computer vision and digital avatars.
 
* Technical Adviser, Xreal (2023 - 2024)
  * As a  Technical Adviser at Xreal, my primary focus is on the research and development of algorithms for 6D object pose estimation, point cloud completion, and 3D foundation models. Working remotely, I collaborate with a team of researchers and engineers to design and implement cutting-edge algorithms that enhance the accuracy and efficiency of computer vision systems.

* Joint Appointment Researcher, TeleAI (2024 - 2025)
  * As a  Joint Appointment Researche, my primary focus is on the research and development of algorithms for llm agent. During the perioad in TeleAI, I collaborate with Dr. Jian Zhao and Prof. Xuelong Li closely.
    
* Remote Algorithm Researcher, Xreal (2021 - 2023)
  * As a Remote Algorithm Researcher at Xreal, my primary focus is on the research and development of algorithms for 6D object pose estimation, point cloud completion, and 3D foundation models. Working remotely, I collaborate with a team of researchers and engineers to design and implement cutting-edge algorithms that enhance the accuracy and efficiency of computer vision systems.
    
* Intern, Bytedance Inc (2020 - 2021)
  * During my internship at Bytedance Inc, I gained valuable experience in the field of computer vision, specifically in the areas of 6D object pose estimation, 3D object detection, and 3D object tracking. As an intern, I actively contributed to the development of algorithms and systems, working closely with experienced professionals in the industry.
 


<br>



Selected Publications
======

<!-- HVG-3D -->
<div class="row" style="display: flex; align-items: center; margin-bottom: 20px;">
  <div class="column left" style="flex: 1;">
    <img align="left" width="100%" src="https://zhaoxinf.github.io/pic/hvg3d.jpg">
  </div>
  <div class="column middle" style="flex: 0.05;">&nbsp;</div>
  <div class="column right" style="flex: 2;">
    <p>
      <strong>HVG-3D: Bridging Real and Simulation Domains for 3D-Conditional Hand-Object Interaction Video Synthesis</strong><br/>
      Mingjin Chen, Junhao Chen, <strong>Zhaoxin Fan</strong>(corresponding author), Yujian Lee, Zichen Dang, Yawen Cui, Lap-Pui Chau, Yi Wang, Lili Wang<br/>
      Conference on Computer Vision and Pattern Recognition (<strong>CVPR</strong>), 2026.<br/>
      [<a href="arxiv.org">Paper</a>]
      [<a href="github.com">Code</a>]
    </p>
  </div>
</div>

<!-- Progress-Think -->
<div class="row" style="display: flex; align-items: center; margin-bottom: 20px;">
  <div class="column left" style="flex: 1;">
    <img align="left" width="100%" src="https://zhaoxinf.github.io/pic/progressthink.jpg">
  </div>
  <div class="column middle" style="flex: 0.05;">&nbsp;</div>
  <div class="column right" style="flex: 2;">
    <p>
      <strong>Progress-Think: Semantic Progress Reasoning for Vision-Language Navigation</strong><br/>
      Shuo Wang, Yucheng Wang, Guoxin Lian, Yongcai Wang, Maiyue Chen, kaihui.wang, Bo Zhang, Zhizhong Su, Zhou Yutian, Wanting Li, Deying Li, <strong>Zhaoxin Fan</strong>(corresponding author)<br/>
      Conference on Computer Vision and Pattern Recognition (<strong>CVPR</strong>), 2026.<br/>
      [<a href="https://arxiv.org/pdf/2511.17097">Paper</a>]
      [<a href="https://horizonrobotics.github.io/robot_lab/progress-think/">Code</a>]
    </p>
  </div>
</div>

<!-- ActAvatar -->
<div class="row" style="display: flex; align-items: center; margin-bottom: 20px;">
  <div class="column left" style="flex: 1;">
    <img align="left" width="100%" src="https://zhaoxinf.github.io/pic/actavatar.jpg">
  </div>
  <div class="column middle" style="flex: 0.05;">&nbsp;</div>
  <div class="column right" style="flex: 2;">
    <p>
      <strong>ActAvatar: Temporally-Aware Precise Action Control for Talking Avatars</strong><br/>
      Ziqiao Peng, Yi Chen, Yifeng Ma, Guozhen Zhang, Zhiyao Sun, Zixiang Zhou, Youliang Zhang, Zhengguang Zhou, Zhaoxin Fan, Hongyan Liu, Yuan Zhou, Qinglin Lu, Jun He<br/>
      Conference on Computer Vision and Pattern Recognition (<strong>CVPR</strong>), 2026.<br/>
      [<a href="https://arxiv.org/pdf/2512.19546">Paper</a>]
      [<a href="https://ziqiaopeng.github.io/ActAvatar/">Code</a>]
    </p>
  </div>
</div>

<!-- CUBic -->
<div class="row" style="display: flex; align-items: center; margin-bottom: 20px;">
  <div class="column left" style="flex: 1;">
    <img align="left" width="100%" src="https://zhaoxinf.github.io/pic/cubic.jpg">
  </div>
  <div class="column middle" style="flex: 0.05;">&nbsp;</div>
  <div class="column right" style="flex: 2;">
    <p>
      <strong>CUBic: Coordinated Unified Bimanual Perception and Control Framework</strong><br/>
      Xingyu Wang, Pengxiang Ding, Jingkai Xu, Donglin Wang, <strong>Zhaoxin Fan</strong>(corresponding author)<br/>
      Conference on Computer Vision and Pattern Recognition (<strong>CVPR</strong>), 2026.<br/>
      [<a href="arxiv.org">Paper</a>]
      [<a href="github.com">Code</a>]
    </p>
  </div>
</div>

<!-- Lyapunov Probes -->
<div class="row" style="display: flex; align-items: center; margin-bottom: 20px;">
  <div class="column left" style="flex: 1;">
    <img align="left" width="100%" src="https://zhaoxinf.github.io/pic/lprobes.jpg">
  </div>
  <div class="column middle" style="flex: 0.05;">&nbsp;</div>
  <div class="column right" style="flex: 2;">
    <p>
      <strong>Lyapunov Probes for Hallucination Detection in Large Foundation Models</strong><br/>
      Bozhi Luan, Gen Li, Yalan Qin, Jifeng Guo, Yun Zhou, Faguo Wu, Hongwei Zheng, <strong>Zhaoxin Fan</strong>(corresponding author), Wenjun Wu<br/>
      Conference on Computer Vision and Pattern Recognition (<strong>CVPR</strong>), 2026.<br/>
      [<a href="arxiv.org">Paper</a>]
      [<a href="github.com">Code</a>]
    </p>
  </div>
</div>

<!-- Erased, But Not Forgotten -->
<div class="row" style="display: flex; align-items: center; margin-bottom: 20px;">
  <div class="column left" style="flex: 1;">
    <img align="left" width="100%" src="https://zhaoxinf.github.io/pic/reflux.jpg">
  </div>
  <div class="column middle" style="flex: 0.05;">&nbsp;</div>
  <div class="column right" style="flex: 2;">
    <p>
      <strong>Erased, But Not Forgotten: Erased Rectified Flow Transformers Still Remain Unsafe Under Concept Attack</strong><br/>
      Nanxiang Jiang,  <strong>Zhaoxin Fan</strong>(corresponding author), Enhan Kang, Daiheng Gao, Yun Zhou, Yanxia Chang, Zheng Zhu, Yeying Jin, Wenjun Wu<br/>
      Conference on Computer Vision and Pattern Recognition (<strong>CVPR (Findings)</strong>) , 2026.<br/>
      [<a href="https://arxiv.org/pdf/2510.00635">Paper</a>]
      [<a href="github.com">Code</a>]
    </p>
  </div>
</div>

<div class="row" style="display: flex; align-items: center; margin-bottom: 20px;">
  <div class="column left" style="flex: 1;">
    <img align="left" width="100%" src="https://zhaoxinf.github.io/pic/heelofllms.jpg">
  </div>
  <div class="column middle" style="flex: 0.05;">&nbsp;</div>
  <div class="column right" style="flex: 2;">
    <p>
      <strong>The Achilles' Heel of LLMs: How Altering a Handful of Neurons Can Cripple Language Abilities</strong><br/>
      Zixuan Qin, Kunlin Lyu, Qingchen Yu, <strong>Zhaoxin Fan</strong>(corresponding author), Yifan Sun<br/>
      International Conference on Learning Representations (<strong>ICLR</strong>), 2026.<br/>
      [<a href="https://arxiv.org/pdf/2510.10238">Paper</a>] 
      [<a href="https://github.com/qqqqqqqzx/The-Achilles-Heel-of-LLMs">Code</a>]
    </p>
  </div>  
</div>


<div class="row" style="display: flex; align-items: center; margin-bottom: 20px;">
  <div class="column left" style="flex: 1;">
    <img align="left" width="100%" src="https://zhaoxinf.github.io/pic/robopara.jpg">
  </div>
  <div class="column middle" style="flex: 0.05;">&nbsp;</div>
  <div class="column right" style="flex: 2;">
    <p>
      <strong>RoboPARA: Dual-Arm Robot Planning with Parallel Allocation and Recomposition Across Tasks</strong><br/>
      Shiying Duan, Pei Ren, Nanxiang Jiang, Zhengping Che, Jian Tang, <strong>Zhaoxin Fan</strong>(corresponding author), Yifan Sun, Wenjun Wu<br/>
      International Conference on Learning Representations (<strong>ICLR</strong>), 2026.<br/>
      [<a href="https://arxiv.org/pdf/2506.06683">Paper</a>] 
      [<a href="https://github.com/AiDuanshiying/RoboPARA">Code</a>]
    </p>
  </div>  
</div>

<div class="row" style="display: flex; align-items: center; margin-bottom: 20px;">
  <div class="column left" style="flex: 1;">
    <img align="left" width="100%" src="https://zhaoxinf.github.io/pic/poserft.jpg">
  </div>
  <div class="column middle" style="flex: 0.05;">&nbsp;</div>
  <div class="column right" style="flex: 2;">
    <p>
      <strong>Pose-RFT: Enhancing MLLMs for 3D Pose Generation via Hybrid Action Reinforcement Fine-Tuning</strong><br/>
      Bao Li, Xiaomei Zhang, Miao Xu, <strong>Zhaoxin Fan</strong>, Xiangyu Zhu, Zhen Lei<br/>
      International Conference on Learning Representations (<strong>ICLR</strong>), 2026.<br/>
      [<a href="https://arxiv.org/pdf/2508.07804" target="_blank">Paper</a>] 
      [<a href="https://github.com/" target="_blank">Code</a>]
    </p>
  </div>
</div>


<div class="row" style="display: flex; align-items: center; margin-bottom: 20px;">
 <div class="column left" style="flex: 1;">
  <img align="left" width="100%" src="https://zhaoxinf.github.io/pic/DSSmoothing.jpg">
</div>
<div class="column middle" style="flex: 0.05;">&nbsp;</div>
<div class="column right" style="flex: 2;">
  <p>
    <strong>DSSmoothing: Toward Certified Dataset Ownership Verification for Pre-trained Language Models via Dual-Space Smoothing</strong><br/>
    Ting Qiao, Xing Liu, Wenke Huang, Jianbin Li, <strong>Zhaoxin Fan</strong>, Yiming Li<br/>
    The Web Conference (<strong>WWW</strong>), 2026.<br/>
    [<a href="https://arxiv.org/pdf/2510.15303" target="_blank">Paper</a>] [<a href="https://github.com/NcepuQiaoTing/DSSmoothing" target="_blank">Code</a>]
  </p>
 </div>
</div>


<div class="row" style="display: flex; align-items: center; margin-bottom: 20px;">
  <div class="column left" style="flex: 1;">
    <img align="left" width="100%" src="https://zhaoxinf.github.io/pic/R-FGDepth.jpg">
  </div>
  <div class="column middle" style="flex: 0.05;">&nbsp;</div>
  <div class="column right" style="flex: 2;">
    <p>
      <strong>R-FGDepth: Towards Foundation Models for Recurrent Depth Learning with Frequency-Guided Initialization and Refinement</strong><br/>
      <strong>Zhaoxin Fan</strong>, Gen Li, Zhongkai Zhou<br/>
      Pattern Recognition (<strong>PR</strong>), 2025.<br/>
      [<a href="https://www.sciencedirect.com/science/article/pii/S0031320325015067">Paper</a>] [<a href="https://github.com/">Code</a>]
    </p>
  </div>  
</div>


<div class="row" style="display: flex; align-items: center; margin-bottom: 20px;">
  <div class="column left" style="flex: 1;">
    <img align="left" width="100%" src="https://zhaoxinf.github.io/pic/mem4d.jpg">
  </div>
  <div class="column middle" style="flex: 0.05;">&nbsp;</div>
  <div class="column right" style="flex: 2;">
    <p>
      <strong>Mem4D: Decoupling Static and Dynamic Memory for Dynamic Scene Reconstruction</strong><br/>
      Xudong Cai, Shuo Wang, Peng Wang, Yongcai Wang, <strong>Zhaoxin Fan</strong> (corresponding author), Wanting Li, Tianbao Zhang, Jianrong Tao, Yeying Jin, Deying Li<br/>
      AAAI Conference on Artificial Intelligence (<strong>AAAI</strong>), 2026.<br/>
      [<a href="https://arxiv.org/pdf/2508.07908">Paper</a>] [<a href="https://github.com/">Code</a>]
    </p>
  </div>  
</div>


<div class="row" style="display: flex; align-items: center; margin-bottom: 20px;">
  <div class="column left" style="flex: 1;">
    <img align="left" width="100%" src="https://zhaoxinf.github.io/pic/monodream.jpg">
  </div>
  <div class="column middle" style="flex: 0.05;">&nbsp;</div>
  <div class="column right" style="flex: 2;">
    <p>
      <strong>MonoDream: Monocular Vision-Language Navigation with Panoramic Dreaming</strong><br/>
      Shuo Wang, Yongcai Wang, Wanting Li, Yucheng Wang, Maiyue Chen, Kaihui Wang, Zhizhong Su, Xudong Cai, Yeying Jin, Deying Li, <strong>Zhaoxin Fan</strong> (corresponding author)<br/>
      AAAI Conference on Artificial Intelligence (<strong>AAAI</strong>), 2026.<br/>
      [<a href="https://arxiv.org/pdf/2508.02549">Paper</a>] [<a href="https://github.com/HorizonRobotics/RoboOrchardLab/tree/master/projects/monodream">Code</a>]
    </p>
  </div>  
</div>

<div class="row" style="display: flex; align-items: center; margin-bottom: 20px;">
  <div class="column left" style="flex: 1;"> 
    <img align="left" width="100%" src="https://zhaoxinf.github.io/pic/synctalk++.jpg"> 
  </div> 
  <div class="column middle" style="flex: 0.05;">&nbsp;</div> 
  <div class="column right" style="flex: 2;"> 
    <p> 
      <strong>SyncTalk++: High-Fidelity and Efficient Synchronized Talking Heads Synthesis Using Gaussian Splatting</strong><br/>
      Ziqiao Peng, Wentao Hu, Junyuan Ma, Xiangyu Zhu, Xiaomei Zhang, Hao Zhao, Hui Tian, Jun He, Hongyan Liu, <strong>Zhaoxin Fan</strong> (corresponding author)<br/> 
      IEEE Transactions on Pattern Analysis and Machine Intelligence (<strong>TPAMI</strong>), 2025.<br/> 
      [<a href="https://arxiv.org/pdf/2506.14742">Paper</a>] [<a href="https://ziqiaopeng.github.io/synctalk++/">Code</a>]
    </p> 
  </div>
</div>



<div class="row" style="display: flex; align-items: center; margin-bottom: 20px;">
  <div class="column left" style="flex: 1;"> 
    <img align="left" width="100%" src="https://zhaoxinf.github.io/pic/asynfusion.jpg"> 
  </div> 
  <div class="column middle" style="flex: 0.05;">&nbsp;</div> 
  <div class="column right" style="flex: 2;"> 
    <p> 
      <strong>AsynFusion: Towards Asynchronous Latent Consistency Models for Decoupled Whole-Body Audio-Driven Avatars</strong><br/>
      Tianbao Zhang, Jian Zhao, Yuer Li, Zheng Zhu, Ping Hu, <strong>Zhaoxin Fan</strong> (corresponding author), Wenjun Wu, Xuelong Li<br/> 
       Chinese Conference on Pattern Recognition and Computer Vision (<strong>PRCV Best Student Paper& CCF outstanding Paper!</strong>), 2025.<br/> 
      [<a href="https://arxiv.org/pdf/2505.15058">Paper</a>] [<a href="#">Code</a>]
    </p> 
  </div>
</div>

<div class="row" style="display: flex; align-items: center; margin-bottom: 20px;">
  <div class="column left" style="flex: 1;">
    <img align="left" width="100%" src="https://zhaoxinf.github.io/pic/auxthink.jpg">
  </div>
  <div class="column middle" style="flex: 0.05;">&nbsp;</div>
  <div class="column right" style="flex: 2;">
    <p>
      <strong>Aux-Think: Exploring Reasoning Strategies for Data-Efficient Vision-Language Navigation</strong><br/>
      Shuo Wang, Yongcai Wang, Wanting Li, Xudong Cai, Yucheng Wang, Maiyue Chen, Kaihui Wang, Zhizhong Su, Deying Li, <strong>Zhaoxin Fan</strong>(corresponding author)<br/>
      Neural Information Processing Systems Conference (<strong>NeurIPS</strong>), 2025.<br/>
      [<a href="https://arxiv.org/pdf/2505.11886">Paper</a>]  [<a href="https://horizonrobotics.github.io/robot_lab/aux-think/">Code</a>]  
    </p>
  </div>
</div>


<div class="row" style="display: flex; align-items: center; margin-bottom: 20px;">
  <div class="column left" style="flex: 1;">
    <img align="left" width="100%" src="https://zhaoxinf.github.io/pic/csur2025.jpg">
  </div>
  <div class="column middle" style="flex: 0.05;">&nbsp;</div>
  <div class="column right" style="flex: 2;">
    <p>
      <strong>A Comprehensive Taxonomy and Analysis of Talking Head Synthesis: Techniques for Portrait Generation, Driving Mechanisms, and Editing</strong><br/>
      Ming Meng, Yufei Zhao, Bo Zhang, Yonggui Zhu, Weimin Shi, Maxwell Wen, <strong>Zhaoxin Fan</strong> (corresponding author)<br/>
      ACM Computing Surveys (<strong>CSUR</strong>), 2025.<br/>
      [<a href="https://arxiv.org/pdf/2406.10553">Paper</a>] [<a href="#">Code</a>]
    </p>
  </div>
</div>

<div class="row" style="display: flex; align-items: center; margin-bottom: 20px;">
  <div class="column left" style="flex: 1;"> 
    <img align="left" width="100%" src="https://zhaoxinf.github.io/pic/Jailbreak.jpg"> 
  </div> 
  <div class="column middle" style="flex: 0.05;">&nbsp;</div> 
  <div class="column right" style="flex: 2;"> 
    <p> 
      <strong>Jailbreak Attack with Multimodal Virtual Scenario Hypnosis for Vision-Language Models</strong><br/>
      Xiayang Shi, Shangfeng Chen, Gang zhang,  <strong>Zhaoxin Fan</strong> (corresponding author), Yinlin Li, Wei Wei, Jingjing Liu<br/> 
      Pattern Recognition (<strong>PR</strong>), 2025.<br/> 
      [<a href="https://www.sciencedirect.com/science/article/pii/S0031320325010520">Paper</a>] [<a href="https://github.com/">Code</a>]
    </p> 
  </div>
</div>


<div class="row" style="display: flex; align-items: center; margin-bottom: 20px;">
  <div class="column left" style="flex: 1;"> 
    <img align="left" width="100%" src="https://zhaoxinf.github.io/pic/longvla.jpg"> 
  </div> 
  <div class="column middle" style="flex: 0.05;">&nbsp;</div> 
  <div class="column right" style="flex: 2;"> 
    <p> 
      <strong>Long-VLA: Unleashing Long-Horizon Capability of Vision Language Action Model for Robot Manipulation</strong><br/>
     Yiguo Fan, Shuanghao Bai, Xinyang Tong, Pengxiang Ding, Yuyang Zhu, Hongchao Lu, Fengqi Dai, Wei Zhao, Yang Liu, Siteng Huang,   <strong>Zhaoxin Fan</strong> , Badong Chen, Donglin Wang<br/> 
      Conference on Robot Learning (<strong>CoRL</strong>), 2025.<br/> 
      [<a href="https://arxiv.org/pdf/2508.19958">Paper</a>] [<a href="https://github.com/">Code</a>]
    </p> 
  </div>
</div>

<div class="row" style="display: flex; align-items: center; margin-bottom: 20px;">
  <div class="column left" style="flex: 1;"> 
    <img align="left" width="100%" src="https://zhaoxinf.github.io/pic/cohedancers.jpg"> 
  </div> 
  <div class="column middle" style="flex: 0.05;">&nbsp;</div> 
  <div class="column right" style="flex: 2;"> 
    <p> 
      <strong>CoheDancers: Enhancing Interactive Group Dance Generation through Music-Driven Coherence Decomposition</strong><br/>
      kaixing yang, XulongTang, Haoyu Wu, Biao Qin, Hongyan Liu, Jun He, <strong>Zhaoxin Fan</strong> (corresponding author)<br/> 
      ACM International Conference on Multimedia (<strong>ACM MM</strong>), 2025.<br/> 
      [<a href="https://arxiv.org/pdf/2412.19123">Paper</a>] [<a href="https://github.com/">Code</a>]
    </p> 
  </div>
</div>


<div class="row" style="display: flex; align-items: center; margin-bottom: 20px;">
  <div class="column left" style="flex: 1;"> 
    <img align="left" width="100%" src="https://zhaoxinf.github.io/pic/flexible.jpg"> 
  </div> 
  <div class="column middle" style="flex: 0.05;">&nbsp;</div> 
  <div class="column right" style="flex: 2;"> 
    <p> 
      <strong>Flexible Multi-view Clustering with Dynamic Views Generation</strong><br/>
      Yalan Qin, Nan Pu, Hanzhou Wu, <strong>Zhaoxin Fan</strong> (corresponding author)<br/> 
      ACM International Conference on Multimedia (<strong>ACM MM</strong>), 2025.<br/> 
      [<a href="https://dl.acm.org/doi/10.1145/3746027.3754930">Paper</a>] [<a href="https://github.com/">Code</a>]
    </p> 
  </div>
</div>


<div class="row" style="display: flex; align-items: center; margin-bottom: 20px;">
  <div class="column left" style="flex: 1;"> 
    <img align="left" width="100%" src="https://zhaoxinf.github.io/pic/MSGM.jpg"> 
  </div> 
  <div class="column middle" style="flex: 0.05;">&nbsp;</div> 
  <div class="column right" style="flex: 2;"> 
    <p> 
      <strong>Moderating the Generalization of Score-based Generative Model</strong><br/>
      Wan Jiang, He Wang, Xin Zhang, Dan Guo,  <strong>Zhaoxin Fan</strong>, Yunfeng Diao, Richang Hong<br/> 
      International Conference on Computer Vision (<strong>ICCV</strong>), 2025.<br/>
      [<a href="https://arxiv.org/pdf/2412.07229">Paper</a>] [<a href="https://github.com/">Code</a>]
    </p> 
  </div>
</div>



<div class="row" style="display: flex; align-items: center; margin-bottom: 20px;">
  <div class="column left" style="flex: 1;"> 
    <img align="left" width="100%" src="https://zhaoxinf.github.io/pic/carp.jpg"> 
  </div> 
  <div class="column middle" style="flex: 0.05;">&nbsp;</div> 
  <div class="column right" style="flex: 2;"> 
    <p> 
      <strong>CARP: Visuomotor Policy Learning via Coarse-to-Fine Autoregressive Prediction</strong><br/>
      Zhefei Gong, Pengxiang Ding, Shangke Lyu, Siteng Huang, Mingyang Sun, Wei Zhao,  <strong>Zhaoxin Fan</strong>, Donglin Wang<br/> 
      International Conference on Computer Vision (<strong>ICCV</strong>), 2025.<br/>
      [<a href="https://arxiv.org/pdf/2412.06782">Paper</a>] [<a href="https://github.com/">Code</a>]
    </p> 
  </div>
</div>


<div class="row" style="display: flex; align-items: center; margin-bottom: 20px;">
  <div class="column left" style="flex: 1;"> 
    <img align="left" width="100%" src="https://zhaoxinf.github.io/pic/tba.jpg"> 
  </div> 
  <div class="column middle" style="flex: 0.05;">&nbsp;</div> 
  <div class="column right" style="flex: 2;"> 
    <p> 
      <strong>Unveiling Hidden Vulnerabilities in Digital Human Generation via Adversarial Attacks</strong><br/>
      Zhiying Li, Yeying Jin, Fan Shen, Zhi Liu, Weibin Chen, Pengju Zhang, Xiaomei Zhang, Boyu Chen, Michael Shen, Kejian Wu, <strong>Zhaoxin Fan</strong> (corresponding author), Jin Dong<br/> 
      Pattern Recognition (<strong>PR</strong>), 2025.<br/> 
      [<a href="https://arxiv.org/pdf/2504.17457">Paper</a>] [<a href="https://github.com/">Code</a>]
    </p> 
  </div>
</div>

<div class="row" style="display: flex; align-items: center; margin-bottom: 20px;">
  <div class="column left" style="flex: 1;"> 
    <img align="left" width="100%" src="https://zhaoxinf.github.io/pic/phys.jpg"> 
  </div> 
  <div class="column middle" style="flex: 0.05;">&nbsp;</div> 
  <div class="column right" style="flex: 2;"> 
    <p> 
      <strong>Phys-EdiGAN: A privacy-preserving method for editing physiological signals in facial videos</strong><br/>
     Xiaoguang Tu, Zhiyi Niu, Juhang Yin, Yanyan Zhang, Ming Yang, Lin Wei, Yu Wang,  <strong>Zhaoxin Fan</strong>, Jian Zhao<br/> 
      Pattern Recognition (<strong>PR</strong>), 2025.<br/> 
      [<a href="https://www.sciencedirect.com/science/article/pii/S0031320325006260">Paper</a>] [<a href="https://github.com/">Code</a>] 
    </p> 
  </div>
</div>


<div class="row" style="display: flex; align-items: center; margin-bottom: 20px;">
  <div class="column left" style="flex: 1;"> 
    <img align="left" width="100%" src="https://zhaoxinf.github.io/pic/saferag.jpg"> 
  </div> 
  <div class="column middle" style="flex: 0.05;">&nbsp;</div> 
  <div class="column right" style="flex: 2;"> 
    <p> 
      <strong>SafeRAG: Benchmarking Security in Retrieval-Augmented Generation of Large Language Model</strong><br/>
      Xun Liang, Simin Niu, Zhiyu Li, Sensen Zhang, Hanyu Wang, Feiyu Xiong, <strong>Zhaoxin Fan</strong>, Bo Tang, Jihao Zhao, Jiawei Yang, Shichao Song, Mengwei Wang<br/> 
      The 63rd Annual Meeting of the Association for Computational Linguistics (<strong>ACL</strong>), 2025.<br/> 
      [<a href="https://arxiv.org/pdf/2501.18636">Paper</a>] [<a href="https://github.com/IAAR-Shanghai/SafeRAG">Code</a>] 
    </p> 
  </div>
</div>

<div class="row" style="display: flex; align-items: center; margin-bottom: 20px;">
  <div class="column left" style="flex: 1;"> 
    <img align="left" width="100%" src="https://zhaoxinf.github.io/pic/moc.jpg"> 
  </div> 
  <div class="column middle" style="flex: 0.05;">&nbsp;</div> 
  <div class="column right" style="flex: 2;"> 
    <p> 
      <strong>MoC: Mixtures of Text Chunking Learners for Retrieval-Augmented Generation System</strong><br/>
      Jihao Zhao, Zhiyuan Ji, <strong>Zhaoxin Fan</strong>, Hanyu Wang, Simin Niu, Bo Tang, Feiyu Xiong, Zhiyu Li<br/> 
      The 63rd Annual Meeting of the Association for Computational Linguistics (<strong>ACL</strong>), 2025.<br/> 
      [<a href="https://arxiv.org/pdf/2503.09600">Paper</a>] [<a href="https://github.com/IAAR-Shanghai/Meta-Chunking/tree/main/MoC">Code</a>] 
    </p> 
  </div>
</div>

<div class="row" style="display: flex; align-items: center; margin-bottom: 20px;">
  <div class="column left" style="flex: 1;"> 
    <img align="left" width="100%" src="https://zhaoxinf.github.io/pic/eraseanything.jpg"> 
  </div> 
  <div class="column middle" style="flex: 0.05;">&nbsp;</div> 
  <div class="column right" style="flex: 2;"> 
    <p> 
      <strong>EraseAnything: Enabling Concept Erasure in Rectified Flow Transformers</strong><br/>
      Daiheng Gao, Shilin Lu, Wenbo Zhou, Jiaming Chu, Jie Zhang, Mengxi Jia, Bang Zhang, <strong>Zhaoxin Fan</strong> (corresponding author), Weiming Zhang<br/> 
      Forty-second International Conference on Machine Learning (<strong>ICML</strong>), 2025.<br/> 
      [<a href="https://arxiv.org/pdf/2412.20413">Paper</a>] [<a href="https://github.com/">Code</a>] 
    </p> 
  </div>
</div>

<div class="row" style="display: flex; align-items: center; margin-bottom: 20px;">
  <div class="column left" style="flex: 1;"> 
    <img align="left" width="100%" src="https://zhaoxinf.github.io/pic/glditalker.jpg"> 
  </div> 
  <div class="column middle" style="flex: 0.05;">&nbsp;</div> 
  <div class="column right" style="flex: 2;"> 
    <p> 
      <strong>GLDiTalker: Speech-Driven 3D Facial Animation with Graph Latent Diffusion Transformer</strong><br/>
      Yihong Lin, <strong>Zhaoxin Fan</strong> (Equal Contribution), Xianjia Wu, Lingyu Xiong, Liang Peng, Xiandong Li, Wenxiong Kang, Songju Lei, Huang Xu<br/> 
      34th International Joint Conference on Artificial Intelligence (<strong>IJCAI</strong>), 2025.<br/> 
      [<a href="https://arxiv.org/pdf/2408.01826">Paper</a>] [<a href="https://github.com/">Code</a>] 
    </p> 
  </div>
</div>



<div class="row" style="display: flex; align-items: center; margin-bottom: 20px;">
  <div class="column left" style="flex: 1;"> 
    <img align="left" width="100%" src="https://zhaoxinf.github.io/pic/mambavo.jpg"> 
  </div> 
  <div class="column middle" style="flex: 0.05;">&nbsp;</div> 
  <div class="column right" style="flex: 2;"> 
    <p> 
      <strong>MambaVO: Deep Visual Odometry Based on Sequential Matching Refinement and Training Smoothing</strong><br/>
      Shuo Wang, Wanting Li, Yongcai Wang, <strong>Zhaoxin Fan</strong> (corresponding author), Zhe Huang, Xudong Cai, Jian Zhao, Deying Li<br/> 
      IEEE Conference on Computer Vision and Pattern Recognition (<strong>CVPR</strong>), 2025.<br/> 
      [<a href="https://arxiv.org/pdf/2412.20082">Paper</a>] [<a href="https://github.com/">Code</a>] 
    </p> 
  </div>
</div>

<div class="row" style="display: flex; align-items: center; margin-bottom: 20px;">
  <div class="column left" style="flex: 1;">
    <img align="left" width="100%" src="https://zhaoxinf.github.io/pic/dualtalk.jpg">
  </div>
  <div class="column middle" style="flex: 0.05;">&nbsp;</div>
  <div class="column right" style="flex: 2;">
    <p>
      <strong>DualTalk: Dual-Speaker Interaction for 3D Talking Head Conversations</strong><br/>
      Ziqiao Peng, Yanbo Fan, Haoyu Wu, Xuan Wang, Hongyan Liu, Jun He, <strong>Zhaoxin Fan</strong> (corresponding author)<br/>
      IEEE Conference on Computer Vision and Pattern Recognition (<strong>CVPR</strong>), 2025.<br/>
      [<a href="https://arxiv.org/pdf/2505.18096">Paper</a>] [<a href="https://github.com/">Code</a>]
    </p>
  </div>
</div>


<div class="row" style="display: flex; align-items: center; margin-bottom: 20px;">
  <div class="column left" style="flex: 1;">
    <img align="left" width="100%" src="https://zhaoxinf.github.io/pic/jtd-uav.jpg">
  </div>
  <div class="column middle" style="flex: 0.05;">&nbsp;</div>
  <div class="column right" style="flex: 2;">
    <p>
      <strong>JTD-UAV: MLLM-Enhanced Joint Tracking and Description Framework for Anti-UAV Systems</strong><br/>
      Yifan Wang, Jian Zhao, <strong>Zhaoxin Fan</strong> (corresponding author), Xin Zhang, Xuecheng Wu, Yudian Zhang, Lei Jin, Xinyue Li, Gang Wang, Mengxi Jia, Ping Hu, Zheng Zhu, Xuelong Li<br/>
      IEEE Conference on Computer Vision and Pattern Recognition (<strong>CVPR</strong>), 2025.<br/>
      [<a href="https://openaccess.thecvf.com/content/CVPR2025/html/Wang_JTD-UAV_MLLM-Enhanced_Joint_Tracking_and_Description_Framework_for_Anti-UAV_Systems_CVPR_2025_paper.html">Paper</a>] [<a href="https://github.com/wangyf2001/MM-AntiUAV/">Code</a>]
    </p>
  </div>
</div>

<div class="row" style="display: flex; align-items: center; margin-bottom: 20px;">
  <div class="column left" style="flex: 1;"> 
    <img align="left" width="100%" src="https://zhaoxinf.github.io/pic/mlphand.jpg"> 
  </div> 
  <div class="column middle" style="flex: 0.05;">&nbsp;</div> 
  <div class="column right" style="flex: 2;"> 
    <p> 
      <strong>MLPHand: Real Time Multi-View 3D Hand Mesh Reconstruction via MLP Modeling</strong><br/>
      Jian Yang, Jiakun Li, Guoming Li, Zhen Shen, Huai-Yu Wu, <strong>Zhaoxin Fan</strong> (corresponding author)<br/> 
      European Conference on Computer Vision (<strong>ECCV</strong>), 2024.<br/> 
      [<a href="https://arxiv.org/pdf/2406.16137">Paper</a>] [<a href="https://github.com/jackyyang9/MLPHand">Code</a>] 
    </p> 
  </div>
</div>

<div class="row" style="display: flex; align-items: center; margin-bottom: 20px;">
  <div class="column left" style="flex: 1;"> 
    <img align="left" width="100%" src="https://zhaoxinf.github.io/pic/synctalk.jpg"> 
  </div> 
  <div class="column middle" style="flex: 0.05;">&nbsp;</div> 
  <div class="column right" style="flex: 2;"> 
    <p> 
      <strong>SyncTalk: The Devil is in the Synchronization for Talking Head Synthesis</strong><br/>
      Ziqiao Peng, Wentao Hu, Yue Shi, Xiangyu Zhu, Xiaomei Zhang, Hao Zhao, Jun He, Hongyan Liu, <strong>Zhaoxin Fan</strong> (corresponding author)<br/> 
      IEEE Conference on Computer Vision and Pattern Recognition (<strong>CVPR</strong>), 2024.<br/> 
      [<a href="https://arxiv.org/pdf/2311.17590.pdf">Paper</a>] [<a href="https://github.com/ZiqiaoPeng/SyncTalk">Code</a>] 
    </p> 
  </div>
</div>

<div class="row" style="display: flex; align-items: center; margin-bottom: 20px;">
  <div class="column left" style="flex: 1;"> 
    <img align="left" width="100%" src="https://zhaoxinf.github.io/pic/monosim.jpg"> 
  </div> 
  <div class="column middle" style="flex: 0.05;">&nbsp;</div> 
  <div class="column right" style="flex: 2;"> 
    <p> 
      <strong>MonoSIM: Simulating Learning Behaviors of Heterogeneous Point Cloud Object Detectors for Monocular 3D Object Detection</strong><br/>
      Han Sun, <strong>Zhaoxin Fan</strong> (equal contribution), Zhenbo Song, Zhicheng Wang, Kejian Wu, and Jianfeng Lu<br/> 
      IEEE Transactions on Instrumentation & Measurement (<strong>TIM</strong>), 2024.<br/> 
      [<a href="https://arxiv.org/pdf/2208.09446.pdf">Paper</a>] [<a href="https://github.com/sunh18/MonoSIM">Code</a>] 
    </p> 
  </div>
</div>

<div class="row" style="display: flex; align-items: center; margin-bottom: 20px;">
  <div class="column left" style="flex: 1;"> 
    <img align="left" width="100%" src="https://zhaoxinf.github.io/pic/everything2motion.jpg"> 
  </div> 
  <div class="column middle" style="flex: 0.05;">&nbsp;</div> 
  <div class="column right" style="flex: 2;"> 
    <p> 
      <strong>Everything2Motion: Synchronizing Diverse Inputs via a Unified Framework for Human Motion Synthesis</strong><br/>
      <strong>Zhaoxin Fan</strong>, Longbin Li, Pengxin Xu, Fan Shen, Kai Chen<br/> 
      Thirty-Eighth AAAI Conference on Artificial Intelligence (<strong>AAAI</strong>), 2024.<br/> 
      [<a href="https://ojs.aaai.org/index.php/AAAI/article/view/27936">Paper</a>] 
    </p> 
  </div>
</div>

<div class="row" style="display: flex; align-items: center; margin-bottom: 20px;">
  <div class="column left" style="flex: 1;"> 
    <img align="left" width="100%" src="https://zhaoxinf.github.io/pic/emotalk.jpg"> 
  </div> 
  <div class="column middle" style="flex: 0.05;">&nbsp;</div> 
  <div class="column right" style="flex: 2;"> 
    <p> 
      <strong>EmoTalk: Speech-driven Emotional Disentanglement for 3D Face Animation</strong><br/>
      Ziqiao Peng, Haoyu Wu, Zhenbo Song, Hao Xu, Xiangyu Zhu, Hongyan Liu, Jun He, <strong>Zhaoxin Fan</strong> (corresponding author)<br/> 
      International Conference on Computer Vision (<strong>ICCV</strong>), 2023.<br/> 
      [<a href="https://openaccess.thecvf.com/content/ICCV2023/papers/Yang_D-IF_Uncertainty-aware_Human_Digitization_via_Implicit_Distribution_Field_ICCV_2023_paper.pdf">Paper</a>] [<a href="https://github.com/psyai-net/EmoTalk_release">Code</a>] 
    </p> 
  </div>
</div>


<div class="row" style="display: flex; align-items: center; margin-bottom: 20px;">
  <div class="column left" style="flex: 1;"> 
    <img align="left" width="100%" src="https://zhaoxinf.github.io/pic/d-if.jpg"> 
  </div> 
  <div class="column middle" style="flex: 0.05;">&nbsp;</div> 
  <div class="column right" style="flex: 2;"> 
    <p> 
      <strong>D-IF: Uncertainty-aware Human Digitization via Implicit Distribution Field</strong><br/>
      Xueting Yang, Yihao Luo, Yuliang Xiu, Wei Wang, Hao Xu, <strong>Zhaoxin Fan</strong> (corresponding author)<br/> 
      International Conference on Computer Vision (<strong>ICCV</strong>), 2023.<br/> 
      [<a href="https://openaccess.thecvf.com/content/ICCV2023/papers/Yang_D-IF_Uncertainty-aware_Human_Digitization_via_Implicit_Distribution_Field_ICCV_2023_paper.pdf">Paper</a>] [<a href="https://github.com/psyai-net/D-IF_release">Code</a>] 
    </p> 
  </div>
</div>

<div class="row" style="display: flex; align-items: center; margin-bottom: 20px;">
  <div class="column left" style="flex: 1;"> 
    <img align="left" width="100%" src="https://zhaoxinf.github.io/pic/selftalk.jpg"> 
  </div> 
  <div class="column middle" style="flex: 0.05;">&nbsp;</div> 
  <div class="column right" style="flex: 2;"> 
    <p> 
      <strong>SelfTalk: A Self-Supervised Commutative Training Diagram to Comprehend 3D Talking Faces</strong><br/>
      Ziqiao Peng, Yihao Luo, Yue Shi, Hao Xu, Xiangyu Zhu, Hongyan Liu, Jun He, <strong>Zhaoxin Fan</strong> (corresponding author)<br/> 
      ACM International Conference on Multimedia (<strong>ACM MM</strong>), 2023.<br/> 
      [<a href="https://arxiv.org/pdf/2306.10799.pdf">Paper</a>] [<a href="https://github.com/psyai-net/SelfTalk_release">Code</a>] 
    </p> 
  </div>
</div>

<div class="row" style="display: flex; align-items: center; margin-bottom: 20px;">
  <div class="column left" style="flex: 1;"> 
    <img align="left" width="100%" src="https://zhaoxinf.github.io/pic/rapd.jpg"> 
  </div> 
  <div class="column middle" style="flex: 0.05;">&nbsp;</div> 
  <div class="column right" style="flex: 2;"> 
    <p> 
      <strong>Reconstruction-Aware Prior Distillation for Semi-supervised Point Cloud Completion</strong><br/>
      <strong>Zhaoxin Fan</strong>, Yulin He, Zhicheng Wang, Kejian Wu, Hongyan Liu, Jun He<br/> 
      International Joint Conference on Artificial Intelligence (<strong>IJCAI</strong>), 2023.<br/> 
      [<a href="https://arxiv.org/pdf/2204.09186.pdf">Paper</a>] 
    </p> 
  </div>
</div>

<div class="row" style="display: flex; align-items: center; margin-bottom: 20px;">
  <div class="column left" style="flex: 1;"> 
    <img align="left" width="100%" src="https://zhaoxinf.github.io/pic/reflection.jpg"> 
  </div> 
  <div class="column middle" style="flex: 0.05;">&nbsp;</div> 
  <div class="column right" style="flex: 2;"> 
    <p> 
      <strong>Robust Single Image Reflection Removal Against Adversarial Attacks</strong><br/>
      Zhenbo Song, Zhenyuan Zhang, Kaihao Zhang, Wenhan Luo, <strong>Zhaoxin Fan</strong>, Wenqi Ren, Jianfeng Lu<br/> 
      IEEE Conference on Computer Vision and Pattern Recognition (<strong>CVPR</strong>), 2023.<br/> 
      [<a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Song_Robust_Single_Image_Reflection_Removal_Against_Adversarial_Attacks_CVPR_2023_paper.pdf">Paper</a>] 
    </p> 
  </div>
</div>

<div class="row" style="display: flex; align-items: center; margin-bottom: 20px;">
  <div class="column left" style="flex: 1;"> 
    <img align="left" width="100%" src="https://zhaoxinf.github.io/pic/old_net.jpg"> 
  </div> 
  <div class="column middle" style="flex: 0.05;">&nbsp;</div> 
  <div class="column right" style="flex: 2;"> 
    <p> 
      <strong>Object Level Depth Reconstruction for Category Level 6D Object Pose Estimation From Monocular RGB Image</strong><br/>
      <strong>Zhaoxin Fan</strong>, Zhenbo Song, Jian Xu, Zhicheng Wang, Kejian Wu, Hongyan Liu, Jun He<br/> 
      European Conference on Computer Vision (<strong>ECCV</strong>), 2022.<br/> 
      [<a href="https://arxiv.org/pdf/2204.01586.pdf">Paper</a>] [<a href="https://github.com/FANzhaoxin666/OLD_Net_release">Code</a>] 
    </p> 
  </div>
</div>

<div class="row" style="display: flex; align-items: center; margin-bottom: 20px;">
  <div class="column left" style="flex: 1;"> 
    <img align="left" width="100%" src="https://zhaoxinf.github.io/pic/svt_net.jpg"> 
  </div> 
  <div class="column middle" style="flex: 0.05;">&nbsp;</div> 
  <div class="column right" style="flex: 2;"> 
    <p> 
      <strong>SVT-Net: Super Light-Weight Sparse Voxel Transformer for Large Scale Place Recognition</strong><br/>
      <strong>Zhaoxin Fan</strong>, Zhenbo Song, Zhiwu Lu, Hongyan Liu, Jun He, Xiaoyong Du<br/> 
      36th AAAI Conference on Artificial Intelligence (<strong>AAAI</strong>), 2022.<br/> 
      [<a href="https://arxiv.org/pdf/2105.00149.pdf">Paper</a>] [<a href="https://github.com/ZhenboSong/svtnet">Code</a>] 
    </p> 
  </div>
</div>

<div class="row" style="display: flex; align-items: center; margin-bottom: 20px;">
  <div class="column left" style="flex: 1;"> 
    <img align="left" width="100%" src="https://zhaoxinf.github.io/pic/pose_tracking.jpg"> 
  </div> 
  <div class="column middle" style="flex: 0.05;">&nbsp;</div> 
  <div class="column right" style="flex: 2;"> 
    <p> 
      <strong>Deep Learning on Monocular Object Pose Detection and Tracking: A Comprehensive Overview</strong><br/>
      <strong>Zhaoxin Fan</strong>, Yazhi Zhu, Yulin He, Qi Sun, Hongyan Liu, Jun He<br/> 
      ACM Computing Surveys (<strong>CSUR</strong>), 2022.<br/> 
      [<a href="https://arxiv.org/pdf/2105.14291.pdf">Paper</a>] 
    </p> 
  </div>
</div>

<div class="row" style="display: flex; align-items: center; margin-bottom: 20px;">
  <div class="column left" style="flex: 1;"> 
    <img align="left" width="100%" src="https://zhaoxinf.github.io/pic/gidp.jpg"> 
  </div> 
  <div class="column middle" style="flex: 0.05;">&nbsp;</div> 
  <div class="column right" style="flex: 2;"> 
    <p> 
      <strong>GIDP: Learning a Good Initialization and Inducing Descriptor Post-enhancing for Large-scale Place Recognition</strong><br/>
      <strong>Zhaoxin Fan</strong>, Zhenbo Song, Hongyan Liu, Jun He<br/> 
      International Conference on Robotics and Automation (<strong>ICRA</strong>), 2023.<br/> 
      [<a href="https://arxiv.org/pdf/2209.11488.pdf">Paper</a>] 
    </p> 
  </div>
</div>


<div class="row" style="display: flex; align-items: center; margin-bottom: 20px;">
  <div class="column left" style="flex: 1;"> 
    <img align="left" width="100%" src="https://zhaoxinf.github.io/pic/srnet_v2.jpg"> 
  </div> 
  <div class="column middle" style="flex: 0.05;">&nbsp;</div> 
  <div class="column right" style="flex: 2;"> 
    <p> 
      <strong>SRNet: A 3D Scene Recognition Network using Static Graph and Dense Semantic Fusion</strong><br/>
      <strong>Zhaoxin Fan</strong>, Hongyan Liu, Jun He, Qi Sun, Xiaoyong Du<br/> 
      Computer Graphics Forum (<strong>CGF</strong>), 2020.<br/> 
      [<a href="https://onlinelibrary.wiley.com/doi/abs/10.1111/cgf.14146">Paper</a>] 
    </p> 
  </div>
</div>

<div class="row" style="display: flex; align-items: center; margin-bottom: 20px;">
  <div class="column left" style="flex: 1;"> 
    <img align="left" width="100%" src="https://zhaoxinf.github.io/pic/graph_one_shot.jpg"> 
  </div> 
  <div class="column middle" style="flex: 0.05;">&nbsp;</div> 
  <div class="column right" style="flex: 2;"> 
    <p> 
      <strong>A Graph‐based One‐Shot Learning Method for Point Cloud Recognition</strong><br/>
      <strong>Zhaoxin Fan</strong>, Hongyan Liu, Jun He, Qi Sun, Xiaoyong Du<br/> 
      Computer Graphics Forum (<strong>CGF</strong>), 2020.<br/> 
      [<a href="https://onlinelibrary.wiley.com/doi/abs/10.1111/cgf.14147">Paper</a>] 
    </p> 
  </div>
</div>

<br>




