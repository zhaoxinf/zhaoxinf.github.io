---
permalink: /
title: ""
excerpt: ""
author_profile: true
redirect_from: 
  - /about/
  - /about.html
---

About me
======
* I am currently working as an assistant professor, collaborating with Prof. [[Zhiming Zheng]](https://iai.buaa.edu.cn/info/1013/1088.htm), Academician of the Chinese Academy of Sciences and Prof. [[Wenjun Wu]](https://iai.buaa.edu.cn/info/1013/1093.htm), at School of Artificial Intelligence, Beihang University.   I am now also working as the chief young scientist of [Psyche AI Inc](https://www.psyai.com/home)（深锶科技）, fosuing on developing advanced AI algorithms for avatars, including avatar driving, avatar generation, and avatar reconsctrution.

* I pursued my PhD from 2019 to 2024 across three prestigious institutions: [Carnegie Mellon University](https://www.cmu.edu)（卡内基梅隆大学）, [The Hong Kong University of Science and Technology](https://www.ust.hk)（香港科技大学）, and [Renmin University of China](https://www.ruc.edu.cn)（中国人民大学）, studying Computer Science (计算机科学). At CMU, I had the privilege of working under the guidance of Prof. [Min Xu](https://xulabs.github.io/min-xu/). My time at HKUST was spent collaborating with Prof. [Kai Chen](https://cse.hkust.edu.hk/~kaichen/). While at RUC, I worked closely with Prof. [Jun He](http://info.ruc.edu.cn/jsky/rtjs/index.htm) and Prof. [Hongyan Liu](https://www.sem.tsinghua.edu.cn/info/1210/32067.htm) from Tsinghua University. My research interests lie in the areas of multi-modal LLMs, avatars, and embodied AI.Please feel free to contact me by  email (fanzhaoxinruc@gmail.com or zhaoxinf@buaa.edu.cn), if you are interested in my research.
  

*  Our team focuses on **human-centric artificial intelligence**, with the ultimate goal of achieving **intelligence that surpasses human capabilities**. Currently, our research spans three key areas: **Avatar**, where we study human interaction and motion from a software perspective; **MLLM**, where we explore human cognition and thought processes through the lens of software; and **Embodied AI**, where we investigate how to transfer the capabilities of avatars and LLMs in virtual environments into the real world, aiming to create **robots that truly exceed human performance**. Through these interconnected efforts, we strive to advance the frontier of artificial intelligence and realize the vision of **superhuman intelligence**.
  


<div class="row" style="display: flex; justify-content: center; margin-bottom: 20px;">
  <div class="column left" style="flex: 1; text-align: center;"> 
    <img style="display: block; margin: 0 auto;" width="80%" src="https://zhaoxinf.github.io/pic/vision_faita.jpg"> 
  </div> 
</div>


* If you are interested in becoming my student or a close collaborator, please answer the following questions before contacting me via email:
  * Firstly, familiarize yourself with my research area by reading my recent papers and visiting our research group's webpage. Research requires passion, and lack of interest may hinder persistence.
  * If you are applying for a direct PhD or a combined Master's and PhD program, it implies that you see academia as a career path. Please decide after understanding the challenges and rewards of an academic life.
  * When applying for a Master's program, I value excellent undergraduate grades, a solid foundation in mathematics, proficient programming skills, and good English proficiency.
  * I hope you are optimistic and proactive, with a resilient spirit, clear logical thinking, and good teamwork skills.
  * After considering these points, if you still wish to choose me as your mentor, please send your resume via email, including personal details, academic records, work or internship experiences, research experience, publications, and any other relevant information. I will respond as soon as possible.
 

*  范肇心，北京航空航天大学人工智能学院助理教授，入选博新计划、北京市青年人才托举工程、北京图像图形学学会优博、CCF智能机器人专委会优博；现为CCF智能机器人专委会执行委员、中国仿真学会算力系统仿真专委会执行委员、中国指挥与控制学会无人系统专委会委员。研究方向包括多模态大模型、数字人和具身智能。在ICML、CVPR、ICCV、ECCV、AAAI、IJCAI、ICRA、PR、TIM、CSUR等人工智能领域顶级期刊和会议上发表论文40余篇，担任PRCV、3DV等国际知名会议领域主席，长期担任TPAMI, IJCV, TIP, TMM, PR, TCSVT, RAL、NeurIPS, CVPR, ICCV, ECCV, AAAI, IJCAI, ACM MM等顶级期刊和会议论文审稿人。作为负责人先后主持或参与国家自然科学基金重点专项项目、北京市自然科学基金青年项目、北航敢为计划青年项目、博士后面上项目、教育部产学研项目等多项国家级及省市级科研项目。范肇心博士注重产业化研究与技术落地，聚焦多模态大模型及智能系统的实际应用。与中国电信、联想集团、国地共建具身智能机器人创新中心、地平线等头部企业长期保持深度合作，其研究成果在数字人、具身智能等领域实现了多项技术转化与工程化应用。 职以来重点研究多模态大模型新型架构以及训练与推理动力学，及其在具身智能和数字人交互领域的应用。

* 现每年合作招收博士生2-3名、硕士生4-6名，并长期招收实习生。研究方向包括多模态大模型、数字人、具身智能及人工智能应用与产业化。博士、硕士招生地点为北京航空航天大学（学院路校区、杭州国际校区）及中国人民大学（北京校区）；实习生可选择线上或在北航杭州国际创新研究院工作。近期招收RA，base香港科技大学，优秀者可转PhD！




<br>

News!
======
*  Oct/25/2025,  Excited to annouce that SyncTalk++ is accepted to **TPAMI** !
*  Oct/16/2025,  Excited to annouce that AsynFusion is awarded by PRCV 2025 **best student paper** and **CCF outstand paper**! 
*  Sep/24/2025,  Glad to annouce that 1 paper is accepted to **Neurips**.
*  Sep/1/2025,  Glad to annouce that 1 paper is accepted to **Pattern Recognition**. The impact factor of PR is 7.6 now.
*  Aug/21//2025,  Glad to annouce that 1 paper is accepted to **PRCV 2025**. 
*  Aug/2/2025,  Excited to annouce that LongVLA is accepted to **CoRL2025** !
*  July/5/2025,  Excited to annouce that 2 papers are accepted to **ACM MM 2025** !
*  June/26/2025,  Excited to annouce that 2 papers are accepted to **ICCV 2025** !
*  June/22/2025,  Glad to annouce that TBA is accepted to **Pattern Recongition**. Our first work on avatar security!
*  June/12/2025,  We will serve as a gestor editor of the **Electronics**, to organize **Special Issue on Advances and Challenges in Multimodal Pattern Recognition**!
*  June/8/2025,  I am happy to serve as the **Area Chair of 3DV 2026**!
*  May/20/2025,  I am happy to serve as the **Area Chair of PRCV 2025**!
*  May/15/2025,  Excited to annouce that 2 papers are accepted to **ACL 2025** !
*  May/1/2025,  EraseAnything is accepted to **ICML 2025！**  Congrats to all co-authors!
*  April/29/2025,  GLDiTalkerg is accepted to **IJCAI 2025！** Congrats to all co-authors!
*  March/30/2025,  We are hiring RAs at HKUST! Welcome to contact us by zhaoxinf@buaa.edu.cn!
*  March/21/2025,  Two paper are accepted to **ICME 2025**. Congrats to Xukun and Zhiying!
*  Feb/27/2025,  Excited to annouce that 3 papers are accepted to **CVPR 2025** !
*  Feb/12/2025,  Excited to annouce that VarGes is  accepted to **CVMJ**. The impact factor of CVMJ is 17.3.
*  Dec/1/2024,  Glad to annouce that Idea-2-3d is accepted to **Coling 2025** main track.
*  July/1/2024,  Glad to annouce that MLPHand is accepted to **ECCV 2024** main track.
*  April/7/2024,  Excited to annouce that 5 papers are accepted to **ICMR 2024**.
*  March/22/2024,  Congrats to Han Sun for his first paper being accepted to **IEEE Transactions on Instrumentation & Measurement.**
*  Feb/27/2024,  Glad to annouce that SyncTalk is accepted to **CVPR 2024** main track.
*  Dec/9/2023,  Our recent paper Everything2Motion is accepted to **AAAI 2024** main track and the paper FurPE is accepted to AAAI 2024 workshop.  Congrats to all authors.
*  Nov/17/2023,  Congrats to Yixing Lu for his first paper being accepted to **International Conference on Multimedia Modeling**.
*  Sep/21/2023,  We are awarded the second prize in the National Challenge Cup! I together with Professor [Li Ronghua](https://ronghuali.github.io/ronghuali.html) serve as the advisors of this project.
*  July/26/2023, I am thrilled to announce that our latest work, SelfTalk, has been accepted by **ACM MM 2023!**
*  Two papers D-IF and Emo-Talk are accepted by **ICCV 2023**, one of the best conference in computer vision.
*  Excited to annouce that one of our  papers has been accepted by **signal processing**, IF=4.729
*  Excited to annouce that one of my papers has been accepted by **IJCAI 2023**.
*  I am honored to have returned to Carnegie Mellon University as a visiting scholar.
*  One of my papers has been accepted for presentation at **CVPR 2023**, which is considered one of the most prestigious computer vision conferences in the world.
*  I am thrilled to have had one of my papers accepted for presentation at **ICRA 2023**, which is a leading robotics conference.
*  My research has been recognized at **ECCV Workshop 2022**, **PRCV 2022**, and **ECCV 2022**, all of which are highly-regarded international conferences in the field of computer vision.
*  I am proud to have had a paper accepted for publication in the **ACM Computing Surveys** journal, which is widely considered one of the best ACM journals with an impact factor of 14.324.
*  The virtual actor An-Ruohan of Psyai, which I helped to develop, performs her show daily on bilibili.
*  My research has also been recognized at **ICIP 2022**, which is a top-tier computer vision conference with a long-standing reputation for excellence.
*  I am excited to announce that one of my papers has been accepted for presentation at **AAAI 2022**, which is a leading artificial intelligence conference.



<br>


Working Experience
======
* Researcher, Psyche AI Inc (2021 - Present)
  * As a researcher at Psyche AI Inc, I am responsible for leading and overseeing various projects related to human body reconstruction, talking head technology, and 3D foundation models. In this role, I contribute my expertise in the development of innovative AI solutions that advance the field of computer vision and digital avatars.
 
* Technical Adviser, Xreal (2023 - Present)
  * As a  Technical Adviser at Xreal, my primary focus is on the research and development of algorithms for 6D object pose estimation, point cloud completion, and 3D foundation models. Working remotely, I collaborate with a team of researchers and engineers to design and implement cutting-edge algorithms that enhance the accuracy and efficiency of computer vision systems.

* Joint Appointment Researcher, TeleAI (2024 - 2025)
  * As a  Joint Appointment Researche, my primary focus is on the research and development of algorithms for llm agent. During the perioad in TeleAI, I collaborate with Dr. Jian Zhao and Prof. Xuelong Li closely.
    
* Remote Algorithm Researcher, Xreal (2021 - 2023)
  * As a Remote Algorithm Researcher at Xreal, my primary focus is on the research and development of algorithms for 6D object pose estimation, point cloud completion, and 3D foundation models. Working remotely, I collaborate with a team of researchers and engineers to design and implement cutting-edge algorithms that enhance the accuracy and efficiency of computer vision systems.
    
* Intern, Bytedance Inc (2020 - 2021)
  * During my internship at Bytedance Inc, I gained valuable experience in the field of computer vision, specifically in the areas of 6D object pose estimation, 3D object detection, and 3D object tracking. As an intern, I actively contributed to the development of algorithms and systems, working closely with experienced professionals in the industry.
 


<br>



Selected Publications
======

<div class="row" style="display: flex; align-items: center; margin-bottom: 20px;">
  <div class="column left" style="flex: 1;"> 
    <img align="left" width="100%" src="https://zhaoxinf.github.io/pic/synctalk++.jpg"> 
  </div> 
  <div class="column middle" style="flex: 0.05;">&nbsp;</div> 
  <div class="column right" style="flex: 2;"> 
    <p> 
      <strong>SyncTalk++: High-Fidelity and Efficient Synchronized Talking Heads Synthesis Using Gaussian Splatting</strong><br/>
      Ziqiao Peng, Wentao Hu, Junyuan Ma, Xiangyu Zhu, Xiaomei Zhang, Hao Zhao, Hui Tian, Jun He, Hongyan Liu, <strong>Zhaoxin Fan</strong> (corresponding author)<br/> 
      IEEE Transactions on Pattern Analysis and Machine Intelligence (<strong>TPAMI</strong>), 2025.<br/> 
      [<a href="https://arxiv.org/pdf/2506.14742">Paper</a>] [<a href="https://ziqiaopeng.github.io/synctalk++/">Code</a>]
    </p> 
  </div>
</div>

<div class="row" style="display: flex; align-items: center; margin-bottom: 20px;">
  <div class="column left" style="flex: 1;"> 
    <img align="left" width="100%" src="https://zhaoxinf.github.io/pic/asynfusion.jpg"> 
  </div> 
  <div class="column middle" style="flex: 0.05;">&nbsp;</div> 
  <div class="column right" style="flex: 2;"> 
    <p> 
      <strong>AsynFusion: Towards Asynchronous Latent Consistency Models for Decoupled Whole-Body Audio-Driven Avatars</strong><br/>
      Tianbao Zhang, Jian Zhao, Yuer Li, Zheng Zhu, Ping Hu, <strong>Zhaoxin Fan</strong> (corresponding author), Wenjun Wu, Xuelong Li<br/> 
       Chinese Conference on Pattern Recognition and Computer Vision (<strong>PRCV Best Student Paper& CCF outstanding Paper!</strong>), 2025.<br/> 
      [<a href="https://arxiv.org/pdf/2505.15058">Paper</a>] [<a href="#">Code</a>]
    </p> 
  </div>
</div>


<div class="row" style="display: flex; align-items: center; margin-bottom: 20px;">
  <div class="column left" style="flex: 1;"> 
    <img align="left" width="100%" src="https://zhaoxinf.github.io/pic/asynfusion.jpg"> 
  </div> 
  <div class="column middle" style="flex: 0.05;">&nbsp;</div> 
  <div class="column right" style="flex: 2;"> 
    <p> 
      <strong>AsynFusion: Towards Asynchronous Latent Consistency Models for Decoupled Whole-Body Audio-Driven Avatars</strong><br/>
      Tianbao Zhang, Jian Zhao, Yuer Li, Zheng Zhu, Ping Hu, <strong>Zhaoxin Fan</strong> (corresponding author), Wenjun Wu, Xuelong Li<br/> 
       Chinese Conference on Pattern Recognition and Computer Vision (<strong>PRCV Best Student Paper& CCF outstanding Paper!</strong>), 2025.<br/> 
      [<a href="https://arxiv.org/pdf/2505.15058">Paper</a>] [<a href="#">Code</a>]
    </p> 
  </div>
</div>

<div class="row" style="display: flex; align-items: center; margin-bottom: 20px;">
  <div class="column left" style="flex: 1;">
    <img align="left" width="100%" src="https://zhaoxinf.github.io/pic/auxthink.jpg">
  </div>
  <div class="column middle" style="flex: 0.05;">&nbsp;</div>
  <div class="column right" style="flex: 2;">
    <p>
      <strong>Aux-Think: Exploring Reasoning Strategies for Data-Efficient Vision-Language Navigation</strong><br/>
      Shuo Wang, Yongcai Wang, Wanting Li, Xudong Cai, Yucheng Wang, Maiyue Chen, Kaihui Wang, Zhizhong Su, Deying Li, <strong>Zhaoxin Fan(corresponding author)</strong><br/>
      Neural Information Processing Systems Conference (<strong>NeurIPS</strong>), 2025.<br/>
      [<a href="https://arxiv.org/pdf/2505.11886">Paper</a>]  [<a href="https://horizonrobotics.github.io/robot_lab/aux-think/">Code</a>]  
    </p>
  </div>
</div>


<div class="row" style="display: flex; align-items: center; margin-bottom: 20px;">
  <div class="column left" style="flex: 1;">
    <img align="left" width="100%" src="https://zhaoxinf.github.io/pic/csur2025.jpg">
  </div>
  <div class="column middle" style="flex: 0.05;">&nbsp;</div>
  <div class="column right" style="flex: 2;">
    <p>
      <strong>A Comprehensive Taxonomy and Analysis of Talking Head Synthesis: Techniques for Portrait Generation, Driving Mechanisms, and Editing</strong><br/>
      Ming Meng, Yufei Zhao, Bo Zhang, Yonggui Zhu, Weimin Shi, Maxwell Wen, <strong>Zhaoxin Fan</strong> (corresponding author)<br/>
      ACM Computing Surveys (<strong>CSUR</strong>), 2025.<br/>
      [<a href="https://arxiv.org/pdf/2406.10553">Paper</a>] [<a href="#">Code</a>]
    </p>
  </div>
</div>

<div class="row" style="display: flex; align-items: center; margin-bottom: 20px;">
  <div class="column left" style="flex: 1;"> 
    <img align="left" width="100%" src="https://zhaoxinf.github.io/pic/Jailbreak.jpg"> 
  </div> 
  <div class="column middle" style="flex: 0.05;">&nbsp;</div> 
  <div class="column right" style="flex: 2;"> 
    <p> 
      <strong>Jailbreak Attack with Multimodal Virtual Scenario Hypnosis for Vision-Language Models</strong><br/>
      Xiayang Shi, Shangfeng Chen, Gang zhang,  <strong>Zhaoxin Fan</strong> (corresponding author), Yinlin Li, Wei Wei, Jingjing Liu<br/> 
      Pattern Recognition (<strong>PR</strong>), 2025.<br/> 
      [<a href="https://www.sciencedirect.com/science/article/pii/S0031320325010520">Paper</a>] [<a href="https://github.com/">Code</a>]
    </p> 
  </div>
</div>


<div class="row" style="display: flex; align-items: center; margin-bottom: 20px;">
  <div class="column left" style="flex: 1;"> 
    <img align="left" width="100%" src="https://zhaoxinf.github.io/pic/longvla.jpg"> 
  </div> 
  <div class="column middle" style="flex: 0.05;">&nbsp;</div> 
  <div class="column right" style="flex: 2;"> 
    <p> 
      <strong>Long-VLA: Unleashing Long-Horizon Capability of Vision Language Action Model for Robot Manipulation</strong><br/>
     Yiguo Fan, Shuanghao Bai, Xinyang Tong, Pengxiang Ding, Yuyang Zhu, Hongchao Lu, Fengqi Dai, Wei Zhao, Yang Liu, Siteng Huang,   <strong>Zhaoxin Fan</strong> , Badong Chen, Donglin Wang<br/> 
      Conference on Robot Learning (<strong>CoRL</strong>), 2025.<br/> 
      [<a href="https://arxiv.org/pdf/2508.19958">Paper</a>] [<a href="https://github.com/">Code</a>]
    </p> 
  </div>
</div>

<div class="row" style="display: flex; align-items: center; margin-bottom: 20px;">
  <div class="column left" style="flex: 1;"> 
    <img align="left" width="100%" src="https://zhaoxinf.github.io/pic/cohedancers.jpg"> 
  </div> 
  <div class="column middle" style="flex: 0.05;">&nbsp;</div> 
  <div class="column right" style="flex: 2;"> 
    <p> 
      <strong>CoheDancers: Enhancing Interactive Group Dance Generation through Music-Driven Coherence Decomposition</strong><br/>
      kaixing yang, XulongTang, Haoyu Wu, Biao Qin, Hongyan Liu, Jun He, <strong>Zhaoxin Fan</strong> (corresponding author)<br/> 
      ACM International Conference on Multimedia (<strong>ACM MM</strong>), 2025.<br/> 
      [<a href="https://arxiv.org/pdf/2412.19123">Paper</a>] [<a href="https://github.com/">Code</a>]
    </p> 
  </div>
</div>


<div class="row" style="display: flex; align-items: center; margin-bottom: 20px;">
  <div class="column left" style="flex: 1;"> 
    <img align="left" width="100%" src="https://zhaoxinf.github.io/pic/flexible.jpg"> 
  </div> 
  <div class="column middle" style="flex: 0.05;">&nbsp;</div> 
  <div class="column right" style="flex: 2;"> 
    <p> 
      <strong>Flexible Multi-view Clustering with Dynamic Views Generation</strong><br/>
      Yalan Qin, Nan Pu, Hanzhou Wu, <strong>Zhaoxin Fan</strong> (corresponding author)<br/> 
      ACM International Conference on Multimedia (<strong>ACM MM</strong>), 2025.<br/> 
      [<a href="https://arxiv.org/pdf/">Paper</a>] [<a href="https://github.com/">Code</a>]
    </p> 
  </div>
</div>


<div class="row" style="display: flex; align-items: center; margin-bottom: 20px;">
  <div class="column left" style="flex: 1;"> 
    <img align="left" width="100%" src="https://zhaoxinf.github.io/pic/MSGM.jpg"> 
  </div> 
  <div class="column middle" style="flex: 0.05;">&nbsp;</div> 
  <div class="column right" style="flex: 2;"> 
    <p> 
      <strong>Moderating the Generalization of Score-based Generative Model</strong><br/>
      Wan Jiang, He Wang, Xin Zhang, Dan Guo,  <strong>Zhaoxin Fan</strong>, Yunfeng Diao, Richang Hong<br/> 
      International Conference on Computer Vision (<strong>ICCV</strong>), 2025.<br/>
      [<a href="https://arxiv.org/pdf/2412.07229">Paper</a>] [<a href="https://github.com/">Code</a>]
    </p> 
  </div>
</div>



<div class="row" style="display: flex; align-items: center; margin-bottom: 20px;">
  <div class="column left" style="flex: 1;"> 
    <img align="left" width="100%" src="https://zhaoxinf.github.io/pic/carp.jpg"> 
  </div> 
  <div class="column middle" style="flex: 0.05;">&nbsp;</div> 
  <div class="column right" style="flex: 2;"> 
    <p> 
      <strong>CARP: Visuomotor Policy Learning via Coarse-to-Fine Autoregressive Prediction</strong><br/>
      Zhefei Gong, Pengxiang Ding, Shangke Lyu, Siteng Huang, Mingyang Sun, Wei Zhao,  <strong>Zhaoxin Fan</strong>, Donglin Wang<br/> 
      International Conference on Computer Vision (<strong>ICCV</strong>), 2025.<br/>
      [<a href="https://arxiv.org/pdf/2412.06782">Paper</a>] [<a href="https://github.com/">Code</a>]
    </p> 
  </div>
</div>


<div class="row" style="display: flex; align-items: center; margin-bottom: 20px;">
  <div class="column left" style="flex: 1;"> 
    <img align="left" width="100%" src="https://zhaoxinf.github.io/pic/tba.jpg"> 
  </div> 
  <div class="column middle" style="flex: 0.05;">&nbsp;</div> 
  <div class="column right" style="flex: 2;"> 
    <p> 
      <strong>Unveiling Hidden Vulnerabilities in Digital Human Generation via Adversarial Attacks</strong><br/>
      Zhiying Li, Yeying Jin, Fan Shen, Zhi Liu, Weibin Chen, Pengju Zhang, Xiaomei Zhang, Boyu Chen, Michael Shen, Kejian Wu, <strong>Zhaoxin Fan</strong> (corresponding author), Jin Dong<br/> 
      Pattern Recognition (<strong>PR</strong>), 2025.<br/> 
      [<a href="https://arxiv.org/pdf/2504.17457">Paper</a>] [<a href="https://github.com/">Code</a>]
    </p> 
  </div>
</div>

<div class="row" style="display: flex; align-items: center; margin-bottom: 20px;">
  <div class="column left" style="flex: 1;"> 
    <img align="left" width="100%" src="https://zhaoxinf.github.io/pic/phys.jpg"> 
  </div> 
  <div class="column middle" style="flex: 0.05;">&nbsp;</div> 
  <div class="column right" style="flex: 2;"> 
    <p> 
      <strong>Phys-EdiGAN: A privacy-preserving method for editing physiological signals in facial videos</strong><br/>
     Xiaoguang Tu, Zhiyi Niu, Juhang Yin, Yanyan Zhang, Ming Yang, Lin Wei, Yu Wang,  <strong>Zhaoxin Fan</strong>, Jian Zhao<br/> 
      Pattern Recognition (<strong>PR</strong>), 2025.<br/> 
      [<a href="https://www.sciencedirect.com/science/article/pii/S0031320325006260">Paper</a>] [<a href="https://github.com/">Code</a>] 
    </p> 
  </div>
</div>


<div class="row" style="display: flex; align-items: center; margin-bottom: 20px;">
  <div class="column left" style="flex: 1;"> 
    <img align="left" width="100%" src="https://zhaoxinf.github.io/pic/saferag.jpg"> 
  </div> 
  <div class="column middle" style="flex: 0.05;">&nbsp;</div> 
  <div class="column right" style="flex: 2;"> 
    <p> 
      <strong>SafeRAG: Benchmarking Security in Retrieval-Augmented Generation of Large Language Model</strong><br/>
      Xun Liang, Simin Niu, Zhiyu Li, Sensen Zhang, Hanyu Wang, Feiyu Xiong, <strong>Zhaoxin Fan</strong>, Bo Tang, Jihao Zhao, Jiawei Yang, Shichao Song, Mengwei Wang<br/> 
      The 63rd Annual Meeting of the Association for Computational Linguistics (<strong>ACL</strong>), 2025.<br/> 
      [<a href="https://arxiv.org/pdf/2501.18636">Paper</a>] [<a href="https://github.com/IAAR-Shanghai/SafeRAG">Code</a>] 
    </p> 
  </div>
</div>

<div class="row" style="display: flex; align-items: center; margin-bottom: 20px;">
  <div class="column left" style="flex: 1;"> 
    <img align="left" width="100%" src="https://zhaoxinf.github.io/pic/moc.jpg"> 
  </div> 
  <div class="column middle" style="flex: 0.05;">&nbsp;</div> 
  <div class="column right" style="flex: 2;"> 
    <p> 
      <strong>MoC: Mixtures of Text Chunking Learners for Retrieval-Augmented Generation System</strong><br/>
      Jihao Zhao, Zhiyuan Ji, <strong>Zhaoxin Fan</strong>, Hanyu Wang, Simin Niu, Bo Tang, Feiyu Xiong, Zhiyu Li<br/> 
      The 63rd Annual Meeting of the Association for Computational Linguistics (<strong>ACL</strong>), 2025.<br/> 
      [<a href="https://arxiv.org/pdf/2503.09600">Paper</a>] [<a href="https://github.com/IAAR-Shanghai/Meta-Chunking/tree/main/MoC">Code</a>] 
    </p> 
  </div>
</div>

<div class="row" style="display: flex; align-items: center; margin-bottom: 20px;">
  <div class="column left" style="flex: 1;"> 
    <img align="left" width="100%" src="https://zhaoxinf.github.io/pic/eraseanything.jpg"> 
  </div> 
  <div class="column middle" style="flex: 0.05;">&nbsp;</div> 
  <div class="column right" style="flex: 2;"> 
    <p> 
      <strong>EraseAnything: Enabling Concept Erasure in Rectified Flow Transformers</strong><br/>
      Daiheng Gao, Shilin Lu, Wenbo Zhou, Jiaming Chu, Jie Zhang, Mengxi Jia, Bang Zhang, c, Weiming Zhang<br/> 
      Forty-second International Conference on Machine Learning (<strong>ICML</strong>), 2025.<br/> 
      [<a href="https://arxiv.org/pdf/2412.20413">Paper</a>] [<a href="https://github.com/">Code</a>] 
    </p> 
  </div>
</div>

<div class="row" style="display: flex; align-items: center; margin-bottom: 20px;">
  <div class="column left" style="flex: 1;"> 
    <img align="left" width="100%" src="https://zhaoxinf.github.io/pic/glditalker.jpg"> 
  </div> 
  <div class="column middle" style="flex: 0.05;">&nbsp;</div> 
  <div class="column right" style="flex: 2;"> 
    <p> 
      <strong>GLDiTalker: Speech-Driven 3D Facial Animation with Graph Latent Diffusion Transformer</strong><br/>
      Yihong Lin, <strong>Zhaoxin Fan</strong> (Equal Contribution), Xianjia Wu, Lingyu Xiong, Liang Peng, Xiandong Li, Wenxiong Kang, Songju Lei, Huang Xu<br/> 
      34th International Joint Conference on Artificial Intelligence (<strong>IJCAI</strong>), 2025.<br/> 
      [<a href="https://arxiv.org/pdf/2408.01826">Paper</a>] [<a href="https://github.com/">Code</a>] 
    </p> 
  </div>
</div>



<div class="row" style="display: flex; align-items: center; margin-bottom: 20px;">
  <div class="column left" style="flex: 1;"> 
    <img align="left" width="100%" src="https://zhaoxinf.github.io/pic/mambavo.jpg"> 
  </div> 
  <div class="column middle" style="flex: 0.05;">&nbsp;</div> 
  <div class="column right" style="flex: 2;"> 
    <p> 
      <strong>MambaVO: Deep Visual Odometry Based on Sequential Matching Refinement and Training Smoothing</strong><br/>
      Shuo Wang, Wanting Li, Yongcai Wang, <strong>Zhaoxin Fan</strong> (corresponding author), Zhe Huang, Xudong Cai, Jian Zhao, Deying Li<br/> 
      IEEE Conference on Computer Vision and Pattern Recognition (<strong>CVPR</strong>), 2025.<br/> 
      [<a href="https://arxiv.org/pdf/2412.20082">Paper</a>] [<a href="https://github.com/">Code</a>] 
    </p> 
  </div>
</div>

<div class="row" style="display: flex; align-items: center; margin-bottom: 20px;">
  <div class="column left" style="flex: 1;">
    <img align="left" width="100%" src="https://zhaoxinf.github.io/pic/dualtalk.jpg">
  </div>
  <div class="column middle" style="flex: 0.05;">&nbsp;</div>
  <div class="column right" style="flex: 2;">
    <p>
      <strong>DualTalk: Dual-Speaker Interaction for 3D Talking Head Conversations</strong><br/>
      Ziqiao Peng, Yanbo Fan, Haoyu Wu, Xuan Wang, Hongyan Liu, Jun He, <strong>Zhaoxin Fan</strong> (corresponding author)<br/>
      IEEE Conference on Computer Vision and Pattern Recognition (<strong>CVPR</strong>), 2025.<br/>
      [<a href="https://arxiv.org/pdf/2505.18096">Paper</a>] [<a href="https://github.com/">Code</a>]
    </p>
  </div>
</div>


<div class="row" style="display: flex; align-items: center; margin-bottom: 20px;">
  <div class="column left" style="flex: 1;">
    <img align="left" width="100%" src="https://zhaoxinf.github.io/pic/jtd-uav.jpg">
  </div>
  <div class="column middle" style="flex: 0.05;">&nbsp;</div>
  <div class="column right" style="flex: 2;">
    <p>
      <strong>JTD-UAV: MLLM-Enhanced Joint Tracking and Description Framework for Anti-UAV Systems</strong><br/>
      Yifan Wang, Jian Zhao, <strong>Zhaoxin Fan</strong> (corresponding author), Xin Zhang, Xuecheng Wu, Yudian Zhang, Lei Jin, Xinyue Li, Gang Wang, Mengxi Jia, Ping Hu, Zheng Zhu, Xuelong Li<br/>
      IEEE Conference on Computer Vision and Pattern Recognition (<strong>CVPR</strong>), 2025.<br/>
      [<a href="https://openaccess.thecvf.com/content/CVPR2025/html/Wang_JTD-UAV_MLLM-Enhanced_Joint_Tracking_and_Description_Framework_for_Anti-UAV_Systems_CVPR_2025_paper.html">Paper</a>] [<a href="https://github.com/wangyf2001/MM-AntiUAV/">Code</a>]
    </p>
  </div>
</div>

<div class="row" style="display: flex; align-items: center; margin-bottom: 20px;">
  <div class="column left" style="flex: 1;"> 
    <img align="left" width="100%" src="https://zhaoxinf.github.io/pic/mlphand.jpg"> 
  </div> 
  <div class="column middle" style="flex: 0.05;">&nbsp;</div> 
  <div class="column right" style="flex: 2;"> 
    <p> 
      <strong>MLPHand: Real Time Multi-View 3D Hand Mesh Reconstruction via MLP Modeling</strong><br/>
      Jian Yang, Jiakun Li, Guoming Li, Zhen Shen, Huai-Yu Wu, <strong>Zhaoxin Fan</strong> (corresponding author)<br/> 
      European Conference on Computer Vision (<strong>ECCV</strong>), 2024.<br/> 
      [<a href="https://arxiv.org/pdf/2406.16137">Paper</a>] [<a href="https://github.com/jackyyang9/MLPHand">Code</a>] 
    </p> 
  </div>
</div>

<div class="row" style="display: flex; align-items: center; margin-bottom: 20px;">
  <div class="column left" style="flex: 1;"> 
    <img align="left" width="100%" src="https://zhaoxinf.github.io/pic/synctalk.jpg"> 
  </div> 
  <div class="column middle" style="flex: 0.05;">&nbsp;</div> 
  <div class="column right" style="flex: 2;"> 
    <p> 
      <strong>SyncTalk: The Devil is in the Synchronization for Talking Head Synthesis</strong><br/>
      Ziqiao Peng, Wentao Hu, Yue Shi, Xiangyu Zhu, Xiaomei Zhang, Hao Zhao, Jun He, Hongyan Liu, <strong>Zhaoxin Fan</strong> (corresponding author)<br/> 
      IEEE Conference on Computer Vision and Pattern Recognition (<strong>CVPR</strong>), 2024.<br/> 
      [<a href="https://arxiv.org/pdf/2311.17590.pdf">Paper</a>] [<a href="https://github.com/ZiqiaoPeng/SyncTalk">Code</a>] 
    </p> 
  </div>
</div>

<div class="row" style="display: flex; align-items: center; margin-bottom: 20px;">
  <div class="column left" style="flex: 1;"> 
    <img align="left" width="100%" src="https://zhaoxinf.github.io/pic/monosim.jpg"> 
  </div> 
  <div class="column middle" style="flex: 0.05;">&nbsp;</div> 
  <div class="column right" style="flex: 2;"> 
    <p> 
      <strong>MonoSIM: Simulating Learning Behaviors of Heterogeneous Point Cloud Object Detectors for Monocular 3D Object Detection</strong><br/>
      Han Sun, <strong>Zhaoxin Fan</strong> (equal contribution), Zhenbo Song, Zhicheng Wang, Kejian Wu, and Jianfeng Lu<br/> 
      IEEE Transactions on Instrumentation & Measurement (<strong>TIM</strong>), 2024.<br/> 
      [<a href="https://arxiv.org/pdf/2208.09446.pdf">Paper</a>] [<a href="https://github.com/sunh18/MonoSIM">Code</a>] 
    </p> 
  </div>
</div>

<div class="row" style="display: flex; align-items: center; margin-bottom: 20px;">
  <div class="column left" style="flex: 1;"> 
    <img align="left" width="100%" src="https://zhaoxinf.github.io/pic/everything2motion.jpg"> 
  </div> 
  <div class="column middle" style="flex: 0.05;">&nbsp;</div> 
  <div class="column right" style="flex: 2;"> 
    <p> 
      <strong>Everything2Motion: Synchronizing Diverse Inputs via a Unified Framework for Human Motion Synthesis</strong><br/>
      <strong>Zhaoxin Fan</strong>, Longbin Li, Pengxin Xu, Fan Shen, Kai Chen<br/> 
      Thirty-Eighth AAAI Conference on Artificial Intelligence (<strong>AAAI</strong>), 2024.<br/> 
      [<a href="https://ojs.aaai.org/index.php/AAAI/article/view/27936">Paper</a>] 
    </p> 
  </div>
</div>

<div class="row" style="display: flex; align-items: center; margin-bottom: 20px;">
  <div class="column left" style="flex: 1;"> 
    <img align="left" width="100%" src="https://zhaoxinf.github.io/pic/emotalk.jpg"> 
  </div> 
  <div class="column middle" style="flex: 0.05;">&nbsp;</div> 
  <div class="column right" style="flex: 2;"> 
    <p> 
      <strong>EmoTalk: Speech-driven Emotional Disentanglement for 3D Face Animation</strong><br/>
      Ziqiao Peng, Haoyu Wu, Zhenbo Song, Hao Xu, Xiangyu Zhu, Hongyan Liu, Jun He, <strong>Zhaoxin Fan</strong> (corresponding author)<br/> 
      International Conference on Computer Vision (<strong>ICCV</strong>), 2023.<br/> 
      [<a href="https://openaccess.thecvf.com/content/ICCV2023/papers/Yang_D-IF_Uncertainty-aware_Human_Digitization_via_Implicit_Distribution_Field_ICCV_2023_paper.pdf">Paper</a>] [<a href="https://github.com/psyai-net/EmoTalk_release">Code</a>] 
    </p> 
  </div>
</div>


<div class="row" style="display: flex; align-items: center; margin-bottom: 20px;">
  <div class="column left" style="flex: 1;"> 
    <img align="left" width="100%" src="https://zhaoxinf.github.io/pic/d-if.jpg"> 
  </div> 
  <div class="column middle" style="flex: 0.05;">&nbsp;</div> 
  <div class="column right" style="flex: 2;"> 
    <p> 
      <strong>D-IF: Uncertainty-aware Human Digitization via Implicit Distribution Field</strong><br/>
      Xueting Yang, Yihao Luo, Yuliang Xiu, Wei Wang, Hao Xu, <strong>Zhaoxin Fan</strong> (corresponding author)<br/> 
      International Conference on Computer Vision (<strong>ICCV</strong>), 2023.<br/> 
      [<a href="https://openaccess.thecvf.com/content/ICCV2023/papers/Yang_D-IF_Uncertainty-aware_Human_Digitization_via_Implicit_Distribution_Field_ICCV_2023_paper.pdf">Paper</a>] [<a href="https://github.com/psyai-net/D-IF_release">Code</a>] 
    </p> 
  </div>
</div>

<div class="row" style="display: flex; align-items: center; margin-bottom: 20px;">
  <div class="column left" style="flex: 1;"> 
    <img align="left" width="100%" src="https://zhaoxinf.github.io/pic/selftalk.jpg"> 
  </div> 
  <div class="column middle" style="flex: 0.05;">&nbsp;</div> 
  <div class="column right" style="flex: 2;"> 
    <p> 
      <strong>SelfTalk: A Self-Supervised Commutative Training Diagram to Comprehend 3D Talking Faces</strong><br/>
      Ziqiao Peng, Yihao Luo, Yue Shi, Hao Xu, Xiangyu Zhu, Hongyan Liu, Jun He, <strong>Zhaoxin Fan</strong> (corresponding author)<br/> 
      ACM International Conference on Multimedia (<strong>ACM MM</strong>), 2023.<br/> 
      [<a href="https://arxiv.org/pdf/2306.10799.pdf">Paper</a>] [<a href="https://github.com/psyai-net/SelfTalk_release">Code</a>] 
    </p> 
  </div>
</div>

<div class="row" style="display: flex; align-items: center; margin-bottom: 20px;">
  <div class="column left" style="flex: 1;"> 
    <img align="left" width="100%" src="https://zhaoxinf.github.io/pic/rapd.jpg"> 
  </div> 
  <div class="column middle" style="flex: 0.05;">&nbsp;</div> 
  <div class="column right" style="flex: 2;"> 
    <p> 
      <strong>Reconstruction-Aware Prior Distillation for Semi-supervised Point Cloud Completion</strong><br/>
      <strong>Zhaoxin Fan</strong>, Yulin He, Zhicheng Wang, Kejian Wu, Hongyan Liu, Jun He<br/> 
      International Joint Conference on Artificial Intelligence (<strong>IJCAI</strong>), 2023.<br/> 
      [<a href="https://arxiv.org/pdf/2204.09186.pdf">Paper</a>] 
    </p> 
  </div>
</div>

<div class="row" style="display: flex; align-items: center; margin-bottom: 20px;">
  <div class="column left" style="flex: 1;"> 
    <img align="left" width="100%" src="https://zhaoxinf.github.io/pic/reflection.jpg"> 
  </div> 
  <div class="column middle" style="flex: 0.05;">&nbsp;</div> 
  <div class="column right" style="flex: 2;"> 
    <p> 
      <strong>Robust Single Image Reflection Removal Against Adversarial Attacks</strong><br/>
      Zhenbo Song, Zhenyuan Zhang, Kaihao Zhang, Wenhan Luo, <strong>Zhaoxin Fan</strong>, Wenqi Ren, Jianfeng Lu<br/> 
      IEEE Conference on Computer Vision and Pattern Recognition (<strong>CVPR</strong>), 2023.<br/> 
      [<a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Song_Robust_Single_Image_Reflection_Removal_Against_Adversarial_Attacks_CVPR_2023_paper.pdf">Paper</a>] 
    </p> 
  </div>
</div>

<div class="row" style="display: flex; align-items: center; margin-bottom: 20px;">
  <div class="column left" style="flex: 1;"> 
    <img align="left" width="100%" src="https://zhaoxinf.github.io/pic/old_net.jpg"> 
  </div> 
  <div class="column middle" style="flex: 0.05;">&nbsp;</div> 
  <div class="column right" style="flex: 2;"> 
    <p> 
      <strong>Object Level Depth Reconstruction for Category Level 6D Object Pose Estimation From Monocular RGB Image</strong><br/>
      <strong>Zhaoxin Fan</strong>, Zhenbo Song, Jian Xu, Zhicheng Wang, Kejian Wu, Hongyan Liu, Jun He<br/> 
      European Conference on Computer Vision (<strong>ECCV</strong>), 2022.<br/> 
      [<a href="https://arxiv.org/pdf/2204.01586.pdf">Paper</a>] [<a href="https://github.com/FANzhaoxin666/OLD_Net_release">Code</a>] 
    </p> 
  </div>
</div>

<div class="row" style="display: flex; align-items: center; margin-bottom: 20px;">
  <div class="column left" style="flex: 1;"> 
    <img align="left" width="100%" src="https://zhaoxinf.github.io/pic/svt_net.jpg"> 
  </div> 
  <div class="column middle" style="flex: 0.05;">&nbsp;</div> 
  <div class="column right" style="flex: 2;"> 
    <p> 
      <strong>SVT-Net: Super Light-Weight Sparse Voxel Transformer for Large Scale Place Recognition</strong><br/>
      <strong>Zhaoxin Fan</strong>, Zhenbo Song, Zhiwu Lu, Hongyan Liu, Jun He, Xiaoyong Du<br/> 
      36th AAAI Conference on Artificial Intelligence (<strong>AAAI</strong>), 2022.<br/> 
      [<a href="https://arxiv.org/pdf/2105.00149.pdf">Paper</a>] [<a href="https://github.com/ZhenboSong/svtnet">Code</a>] 
    </p> 
  </div>
</div>

<div class="row" style="display: flex; align-items: center; margin-bottom: 20px;">
  <div class="column left" style="flex: 1;"> 
    <img align="left" width="100%" src="https://zhaoxinf.github.io/pic/pose_tracking.jpg"> 
  </div> 
  <div class="column middle" style="flex: 0.05;">&nbsp;</div> 
  <div class="column right" style="flex: 2;"> 
    <p> 
      <strong>Deep Learning on Monocular Object Pose Detection and Tracking: A Comprehensive Overview</strong><br/>
      <strong>Zhaoxin Fan</strong>, Yazhi Zhu, Yulin He, Qi Sun, Hongyan Liu, Jun He<br/> 
      ACM Computing Surveys (<strong>CSUR</strong>), 2022.<br/> 
      [<a href="https://arxiv.org/pdf/2105.14291.pdf">Paper</a>] 
    </p> 
  </div>
</div>

<div class="row" style="display: flex; align-items: center; margin-bottom: 20px;">
  <div class="column left" style="flex: 1;"> 
    <img align="left" width="100%" src="https://zhaoxinf.github.io/pic/gidp.jpg"> 
  </div> 
  <div class="column middle" style="flex: 0.05;">&nbsp;</div> 
  <div class="column right" style="flex: 2;"> 
    <p> 
      <strong>GIDP: Learning a Good Initialization and Inducing Descriptor Post-enhancing for Large-scale Place Recognition</strong><br/>
      <strong>Zhaoxin Fan</strong>, Zhenbo Song, Hongyan Liu, Jun He<br/> 
      International Conference on Robotics and Automation (<strong>ICRA</strong>), 2023.<br/> 
      [<a href="https://arxiv.org/pdf/2209.11488.pdf">Paper</a>] 
    </p> 
  </div>
</div>


<div class="row" style="display: flex; align-items: center; margin-bottom: 20px;">
  <div class="column left" style="flex: 1;"> 
    <img align="left" width="100%" src="https://zhaoxinf.github.io/pic/srnet1.jpg"> 
  </div> 
  <div class="column middle" style="flex: 0.05;">&nbsp;</div> 
  <div class="column right" style="flex: 2;"> 
    <p> 
      <strong>SRNet: A 3D Scene Recognition Network using Static Graph and Dense Semantic Fusion</strong><br/>
      <strong>Zhaoxin Fan</strong>, Hongyan Liu, Jun He, Qi Sun, Xiaoyong Du<br/> 
      Computer Graphics Forum (<strong>CGF</strong>), 2020.<br/> 
      [<a href="https://onlinelibrary.wiley.com/doi/abs/10.1111/cgf.14146">Paper</a>] 
    </p> 
  </div>
</div>

<div class="row" style="display: flex; align-items: center; margin-bottom: 20px;">
  <div class="column left" style="flex: 1;"> 
    <img align="left" width="100%" src="https://zhaoxinf.github.io/pic/graph_one_shot.jpg"> 
  </div> 
  <div class="column middle" style="flex: 0.05;">&nbsp;</div> 
  <div class="column right" style="flex: 2;"> 
    <p> 
      <strong>A Graph‐based One‐Shot Learning Method for Point Cloud Recognition</strong><br/>
      <strong>Zhaoxin Fan</strong>, Hongyan Liu, Jun He, Qi Sun, Xiaoyong Du<br/> 
      Computer Graphics Forum (<strong>CGF</strong>), 2020.<br/> 
      [<a href="https://onlinelibrary.wiley.com/doi/abs/10.1111/cgf.14147">Paper</a>] 
    </p> 
  </div>
</div>

<br>




